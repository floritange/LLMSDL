id,APP,commit url,types,cases,explanation,consequences,source-code locations,defect-triggering tests,correct source-code paths,consequences_details
0,Shaunwei/RealChar,https://github.com/Shaunwei/RealChar/tree/ee36a803e220c8a3f021a61922ea81c59e7242ee,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,RealChar/realtime_ai_character/llm/openai_llm.py,"1.In the RealChar UI, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""",['realtime_ai_character/llm/openai_llm.py'],['incorrectness']
1,Shaunwei/RealChar,https://github.com/Shaunwei/RealChar/tree/ee36a803e220c8a3f021a61922ea81c59e7242ee,lacking restrictions in prompt,case1,"When the user conducts a chat test with a custom character, the character admits to being a language model (GPT-3.5).",IC,"1.realtime_ai_character/llm/__init__.py /#13
2.realtime_ai_character/llm/openai_llm.py","1. Use a custom character with change only being made to character description
2. Ask the character: ""There is no AI, much less computer during your existence.How do you know about it?""","['realtime_ai_character/llm/__init__.py', 'realtime_ai_character/llm/openai_llm.py']",['incorrectness']
2,Shaunwei/RealChar,https://github.com/Shaunwei/RealChar/tree/ee36a803e220c8a3f021a61922ea81c59e7242ee,insufficient history management,/,lack a chat history feature and characters can summarize previous conversations.,IC,realtime_ai_character/models/interaction.py,"1.In the RealChar UI, select a character to converse with.
2.Ask the character to summarize the conversation, but the character is unable to do so.",['realtime_ai_character/models/interaction.py'],['incorrectness']
3,eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,unclear context in prompt,case1,"When using ""Chat Knowledge"", answers may include information not present in the knowledge documents, often resulting in fabricated or nonsensical responses.",IC,"dbgpt/app/scene/chat_knowledge/chat.py, prompt.py","1.Enter the ""Chat Knowledge"" scene
2.Upload knowledge documents in md format
3.Start the conversation","['pilot/scene/chat_knowledge/url/chat.py', 'pilot/scene/chat_knowledge/custom/chat.py', 'pilot/scene/chat_knowledge/custom/prompt.py', 'pilot/scene/chat_knowledge/inner_db_summary/prompt.py', 'pilot/scene/chat_knowledge/inner_db_summary/chat.py', 'pilot/scene/chat_knowledge/default/chat.py', 'pilot/scene/chat_knowledge/default/prompt.py', 'pilot/scene/chat_knowledge/url/prompt.py']",['incorrectness']
4,eosphoros-ai/DB-GPT,https://github.com/eosphoros-ai/DB-GPT/tree/1519df7f0179be0957008e75fd27eafb9d6890f1,exceeding llm content limit,case1,"Multi-turn dialogue configuration is not supported. The parameter need_historical_messages=True is set, but it does not work. The model is qwen72b.",UI,"dbgpt/app/scene/chat_db/auto_execute/prompt.py, chat.py","
1.Download and configure DB-GPT.
2.Set need_historical_messages=True in the dbgpt/app/scene/base.py file.
3.Start a conversation test using the qwen72b model and find that multi-turn dialogue is not supported.","['pilot/scene/chat_db/auto_execute/prompt.py', 'pilot/scene/chat_db/auto_execute/chat.py']",['unfriendly user interface']
5,X-D-Lab/LangChain-ChatGLM-Webui,https://github.com/X-D-Lab/LangChain-ChatGLM-Webui/tree/ef829a28234228761a97541e4ebae9da4f4e6800,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"app.py
modelscope/chatglm_llm.py","1.In the LangChain-ChatGLM-Webui, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""","['modelscope/chatglm_llm.py', 'app.py']",['incorrectness']
6,X-D-Lab/LangChain-ChatGLM-Webui,https://github.com/X-D-Lab/LangChain-ChatGLM-Webui/tree/ef829a28234228761a97541e4ebae9da4f4e6800,missing llm input format validation,/,Unable to simultaneously upload multiple documents of the same format or multiple documents of different formats.,SL,"1.LangChain-ChatGLM-Webui/app.py/class KnowledgeBasedChatLLM/load_file()
2.LangChain-ChatGLM-Webui/app.py/class KnowledgeBasedChatLLM/init_vector_store()
3..LangChain-ChatGLM-Webui/app.py/the Gradio UI section/""file = gr.File(label='请上传知识库文件', file_types=['.txt', '.md', '.docx', '.pdf'])""","1.Upload the knowledge base file on the left side of the project's UI interface.
2.After uploading one file, the user cannot upload any additional files.",['app.py'],['slower execution']
7,X-D-Lab/LangChain-ChatGLM-Webui,https://github.com/X-D-Lab/LangChain-ChatGLM-Webui/tree/ef829a28234228761a97541e4ebae9da4f4e6800,exceeding llm content limit,/,Unable to simultaneously upload multiple documents of the same format or multiple documents of different formats.,IC,"paddlepaddle/chatllm.py
chatllm.py",simultaneously upload multiple documents of the same format or multiple documents of different formats.,['paddlepaddle/chatllm.py'],['incorrectness']
8,X-D-Lab/LangChain-ChatGLM-Webui,https://github.com/X-D-Lab/LangChain-ChatGLM-Webui/tree/ef829a28234228761a97541e4ebae9da4f4e6800,conflicting knowledge entries,/,"
When a new file is uploaded, the knowledge from the previous file is overwritten. users want to enable multiple files to be uploaded sequentially, accumulating and combining into the knowledge base.",IC,"1.LangChain-ChatGLM-Webui/app.py/class KnowledgeBasedChatLLM/load_file()
2.LangChain-ChatGLM-Webui/app.py/class KnowledgeBasedChatLLM/init_vector_store()
3..LangChain-ChatGLM-Webui/app.py/the Gradio UI section/""file = gr.File(label='请上传知识库文件', file_types=['.txt', '.md', '.docx', '.pdf'])""","(same as bug1:Unable to simultaneously upload multiple documents of the same format or multiple documents of different formats.)
1.Upload a knowledge base file on the left side of the project's UI interface.
2.After uploading one file, users cannot upload another file without deleting the previously uploaded file.",['app.py'],['incorrectness']
9,X-D-Lab/LangChain-ChatGLM-Webui,https://github.com/X-D-Lab/LangChain-ChatGLM-Webui/tree/ef829a28234228761a97541e4ebae9da4f4e6800,inefficient memory management,/,"User specified the program to use the GPU with the least memory usage at startup. However, upon reloading the model, resources from the old model are not released, and the new model loads on the default device instead of the specified GPU.",UI,"1.LangChain-ChatGLM-Webui/app.py/class KnowledgeBasedChatLLM/init_model_config()
2.LangChain-ChatGLM-Webui/requirements.txt/ Update the versions of some packages (langchain==0.1.0,
transformers==4.30.2, wandb==0.16.2, protobuf==4.25.2, langchain-community==0.0.11 )
3.LangChain-ChatGLM-Webui/app.py/some modifications to library imports: 
""from langchain_community.document_loaders import UnstructuredFileLoader""
""from langchain_community.vectorstores import FAISS""","1.Verify that the system has multiple GPUs available.
2.Execute a command to determine the GPU with the least memory consumption before launching the program. This command typically looks like:
""os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp')
memory_gpu = [int(x.split()[2]) for x in open('tmp', 'r').readlines()]
DEVICE_ID = np.argmax(memory_gpu)
torch.cuda.set_device(int(DEVICE_ID))""
3.Launch the program. Upon startup, the default model ChatGLM-6B-int4 is loaded successfully, and the program shows device=3.
4.Select the ChatGLM-6B-int8 model for reloading. However, an error occurs during the reloading process.The specific error message indicates: ""CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 31.75 GiB total capacity; 4.25 GiB already allocated; 44.75 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""
5.To sum up,the issues are :
*Resources occupied by the old model are not released after reloading.
*The new model is not loaded onto the GPU with device ID 3 but instead uses the default device, which is GPU 0.",['app.py'],['unfriendly user interface']
10,langchain-ai/chat-langchain,https://github.com/langchain-ai/chat-langchain/tree/a875a649109ad7c3d68d7e9b75508f687e627ca4,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,_scripts/evaluate_chains_agent.py,Upload the pptx file.,['_scripts/evaluate_chains_agent.py'],['incorrectness']
11,langchain-ai/chat-langchain,https://github.com/langchain-ai/chat-langchain/tree/a875a649109ad7c3d68d7e9b75508f687e627ca4,exceeding llm content limit,/,Unable to simultaneously upload multiple documents of the same format or multiple documents of different formats.,IC,_scripts/evaluate_chains_agent.py,simultaneously upload multiple documents of the same format or multiple documents of different formats.,['_scripts/evaluate_chains_agent.py'],['incorrectness']
12,hamelsmu/chat-langchain,https://github.com/hamelsmu/chat-langchain/tree/8252cc469335da08230afb049cdc7dfb178c328b,unclear context in prompt,case1,"When user ask questions,some irrelevant sources also come.",IC,query_data.py,"1.Ask a question in the UI interface.
2.Notice that the returned sources contain content unrelated to the question.",['query_data.py'],['incorrectness']
13,hamelsmu/chat-langchain,https://github.com/hamelsmu/chat-langchain/tree/8252cc469335da08230afb049cdc7dfb178c328b,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,ingest.py,Upload the pptx file.,['ingest.py'],['incorrectness']
14,hamelsmu/chat-langchain,https://github.com/hamelsmu/chat-langchain/tree/8252cc469335da08230afb049cdc7dfb178c328b,unnecessary llm output,/,Running locally. Tthe answer strings displayed in the frontend contain repeated words.,IC,main.py,"1.Set up chat-langchain locally.
2.Open the browser interface and ask a question.
3.Notice that the answers displayed in the browser have duplicated words (e.g., ""CanCan you you provide provide me me with with instructions instructions on on how how to to..."").
4.Debug backtrace from smith shows the response works fine, but the issue persists in the browser display.",['main.py'],['incorrectness']
15,hamelsmu/chat-langchain,https://github.com/hamelsmu/chat-langchain/tree/8252cc469335da08230afb049cdc7dfb178c328b,exceeding llm content limit,/,Unable to simultaneously upload multiple documents of the same format or multiple documents of different formats.,IC,"query_data.py
callback.py",simultaneously upload multiple documents of the same format or multiple documents of different formats.,"['callback.py', 'query_data.py']",['incorrectness']
16,joaocarlosleme/chat-langchain,https://github.com/joaocarlosleme/chat-langchain/tree/362e71c016d70022a6b1d067e0cddbe1a6ef496e,unclear context in prompt,case1,"When user ask questions,some irrelevant sources also come.",IC,query_data.py,"1.Ask a question in the UI interface.
2.Notice that the returned sources contain content unrelated to the question.",['query_data.py'],['incorrectness']
17,joaocarlosleme/chat-langchain,https://github.com/joaocarlosleme/chat-langchain/tree/362e71c016d70022a6b1d067e0cddbe1a6ef496e,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,ingest.py,Upload the pptx file.,['ingest.py'],['incorrectness']
18,joaocarlosleme/chat-langchain,https://github.com/joaocarlosleme/chat-langchain/tree/362e71c016d70022a6b1d067e0cddbe1a6ef496e,unnecessary llm output,/,Running locally. Tthe answer strings displayed in the frontend contain repeated words.,IC,main.py,"1.Set up chat-langchain locally.
2.Open the browser interface and ask a question.
3.Notice that the answers displayed in the browser have duplicated words (e.g., ""CanCan you you provide provide me me with with instructions instructions on on how how to to..."").
4.Debug backtrace from smith shows the response works fine, but the issue persists in the browser display.",['main.py'],['incorrectness']
19,blu3mo/scrapchat,https://github.com/blu3mo/scrapchat/tree/ce78a875399b069d3d5c44c738cb10fb3704c0fd,unclear context in prompt,case1,"When user ask questions,some irrelevant sources also come.",IC,"query_data.py
archive/chain.py","1.Ask a question in the UI interface.
2.Notice that the returned sources contain content unrelated to the question.","['archive/chain.py', 'query_data.py']",['incorrectness']
20,blu3mo/scrapchat,https://github.com/blu3mo/scrapchat/tree/ce78a875399b069d3d5c44c738cb10fb3704c0fd,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,"archive/app.py
archive/ingest_examples.py",Upload the pptx file.,"['archive/app.py', 'archive/ingest_examples.py']",['incorrectness']
21,blu3mo/scrapchat,https://github.com/blu3mo/scrapchat/tree/ce78a875399b069d3d5c44c738cb10fb3704c0fd,unnecessary llm output,/,Running locally. Tthe answer strings displayed in the frontend contain repeated words.,IC,"query_data.py
archive/chain.py
callback.py","1.Set up chat-langchain locally.
2.Open the browser interface and ask a question.
3.Notice that the answers displayed in the browser have duplicated words (e.g., ""CanCan you you provide provide me me with with instructions instructions on on how how to to..."").
4.Debug backtrace from smith shows the response works fine, but the issue persists in the browser display.","['archive/chain.py', 'query_data.py']",['incorrectness']
22,dory111111/babyagi-streamlit,https://github.com/dory111111/babyagi-streamlit/tree/088aa7376c39d541543a3518df8bd5db46633f51,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi.py/task_creation_chain,"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks.",['babyagi.py'],"['incorrectness', 'slower execution']"
23,dory111111/babyagi-streamlit,https://github.com/dory111111/babyagi-streamlit/tree/088aa7376c39d541543a3518df8bd5db46633f51,incompatible llm output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder.",['babyagi.py'],['incorrectness']
24,dory111111/babyagi-streamlit,https://github.com/dory111111/babyagi-streamlit/tree/088aa7376c39d541543a3518df8bd5db46633f51,unnecessary llm output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi.py/task_creation_agent(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes",['babyagi.py'],"['incorrectness', 'unfriendly user interface']"
25,dory111111/babyagi-streamlit,https://github.com/dory111111/babyagi-streamlit/tree/088aa7376c39d541543a3518df8bd5db46633f51,imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi.py/execution_agent(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py",['babyagi.py'],"['incorrectness', 'slower execution']"
26,dory111111/babyagi-streamlit,https://github.com/dory111111/babyagi-streamlit/tree/088aa7376c39d541543a3518df8bd5db46633f51,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,"babyagi.py/execution_agent()/class SingleTaskListStorage, class DefaultResultsStorage","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available.",['babyagi.py'],['unfriendly user interface']
27,kroll-software/babyagi4all,https://github.com/kroll-software/babyagi4all/tree/3dc42659eb5cb51dbea71b565ce09f61a130cc20,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi.py/task_creation_agent(),"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks.",['babyagi.py'],"['incorrectness', 'slower execution']"
28,kroll-software/babyagi4all,https://github.com/kroll-software/babyagi4all/tree/3dc42659eb5cb51dbea71b565ce09f61a130cc20,incompatible llm output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder.",['babyagi.py'],['incorrectness']
29,kroll-software/babyagi4all,https://github.com/kroll-software/babyagi4all/tree/3dc42659eb5cb51dbea71b565ce09f61a130cc20,unnecessary llm output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi.py/task_creation_agent(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes",['babyagi.py'],"['incorrectness', 'unfriendly user interface']"
30,kroll-software/babyagi4all,https://github.com/kroll-software/babyagi4all/tree/3dc42659eb5cb51dbea71b565ce09f61a130cc20,imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi.py/execution_agent(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py",['babyagi.py'],"['incorrectness', 'slower execution']"
31,kroll-software/babyagi4all,https://github.com/kroll-software/babyagi4all/tree/3dc42659eb5cb51dbea71b565ce09f61a130cc20,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,"babyagi.py/execution_agent()/class SingleTaskListStorage, class DefaultResultsStorage","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available.",['babyagi.py'],['unfriendly user interface']
32,sw5park/LUISE,https://github.com/sw5park/LUISE/tree/e88343c1e2d1c634a33838500e6cc72aa97fdeaa,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",scripts/main.py/task_creation_agent(),"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks.",['scripts/main.py'],"['incorrectness', 'slower execution']"
33,sw5park/LUISE,https://github.com/sw5park/LUISE/tree/e88343c1e2d1c634a33838500e6cc72aa97fdeaa,incompatible llm output format,/,Task list numbering continues to reset or get misordered.,IC,"scripts/main.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder.",['scripts/main.py'],['incorrectness']
34,sw5park/LUISE,https://github.com/sw5park/LUISE/tree/e88343c1e2d1c634a33838500e6cc72aa97fdeaa,unnecessary llm output,/,The task creation process generating too many unnecessary tasks,"IC,UI",scripts/main.py/task_creation_agent(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes",['scripts/main.py'],"['incorrectness', 'unfriendly user interface']"
35,sw5park/LUISE,https://github.com/sw5park/LUISE/tree/e88343c1e2d1c634a33838500e6cc72aa97fdeaa,imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",scripts/main.py/execution_agent(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py",['scripts/main.py'],"['incorrectness', 'slower execution']"
36,sw5park/LUISE,https://github.com/sw5park/LUISE/tree/e88343c1e2d1c634a33838500e6cc72aa97fdeaa,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,"scripts/main.py/execution_agent()/class SingleTaskListStorage, class DefaultResultsStorage","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available.",['scripts/main.py'],['unfriendly user interface']
37,robiwan303/babyagi,https://github.com/robiwan303/babyagi/tree/9c22f053675710094576bfc4e6527f59a50e6ac2,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi.py/task_creation_agent(),"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks.",['babyagi.py'],"['incorrectness', 'slower execution']"
38,robiwan303/babyagi,https://github.com/robiwan303/babyagi/tree/9c22f053675710094576bfc4e6527f59a50e6ac2,incompatible llm output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder.",['babyagi.py'],['incorrectness']
39,robiwan303/babyagi,https://github.com/robiwan303/babyagi/tree/9c22f053675710094576bfc4e6527f59a50e6ac2,unnecessary llm output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi.py/task_creation_agent(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes",['babyagi.py'],"['incorrectness', 'unfriendly user interface']"
40,robiwan303/babyagi,https://github.com/robiwan303/babyagi/tree/9c22f053675710094576bfc4e6527f59a50e6ac2,imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi.py/execution_agent(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py",['babyagi.py'],"['incorrectness', 'slower execution']"
41,robiwan303/babyagi,https://github.com/robiwan303/babyagi/tree/9c22f053675710094576bfc4e6527f59a50e6ac2,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,"babyagi.py/execution_agent()/class SingleTaskListStorage, class DefaultResultsStorage","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available.",['babyagi.py'],['unfriendly user interface']
42,saten-private/BabyCommandAGI,https://github.com/saten-private/BabyCommandAGI/tree/993f7075479d4d89948410bfec0c4c18d4a06b0c,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",classic/babyagi.py/task_creation_agent(),"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks.",['classic/babyagi.py'],"['incorrectness', 'slower execution']"
43,saten-private/BabyCommandAGI,https://github.com/saten-private/BabyCommandAGI/tree/993f7075479d4d89948410bfec0c4c18d4a06b0c,incompatible llm output format,/,Task list numbering continues to reset or get misordered.,IC,"classic/babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder.",['classic/babyagi.py'],['incorrectness']
44,saten-private/BabyCommandAGI,https://github.com/saten-private/BabyCommandAGI/tree/993f7075479d4d89948410bfec0c4c18d4a06b0c,unnecessary llm output,/,The task creation process generating too many unnecessary tasks,"IC,UI",classic/babyagi.py/task_creation_agent(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes",['classic/babyagi.py'],"['incorrectness', 'unfriendly user interface']"
45,saten-private/BabyCommandAGI,https://github.com/saten-private/BabyCommandAGI/tree/993f7075479d4d89948410bfec0c4c18d4a06b0c,imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",classic/babyagi.py/execution_agent(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py",['classic/babyagi.py'],"['incorrectness', 'slower execution']"
46,saten-private/BabyCommandAGI,https://github.com/saten-private/BabyCommandAGI/tree/993f7075479d4d89948410bfec0c4c18d4a06b0c,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,"classic/babyagi.py/execution_agent()/class SingleTaskListStorage, class DefaultResultsStorage","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available.",['classic/babyagi.py'],['unfriendly user interface']
47,ai8hyf/babyagi,https://github.com/ai8hyf/babyagi/tree/6fcd528a92c80846dbb351f2b7babdd50c38709d,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi.py/task_creation_agent(),"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks.",['babyagi.py'],"['incorrectness', 'slower execution']"
48,ai8hyf/babyagi,https://github.com/ai8hyf/babyagi/tree/6fcd528a92c80846dbb351f2b7babdd50c38709d,incompatible llm output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder.",['babyagi.py'],['incorrectness']
49,ai8hyf/babyagi,https://github.com/ai8hyf/babyagi/tree/6fcd528a92c80846dbb351f2b7babdd50c38709d,unnecessary llm output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi.py/task_creation_agent(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes",['babyagi.py'],"['incorrectness', 'unfriendly user interface']"
50,ai8hyf/babyagi,https://github.com/ai8hyf/babyagi/tree/6fcd528a92c80846dbb351f2b7babdd50c38709d,imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi.py/execution_agent(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py",['babyagi.py'],"['incorrectness', 'slower execution']"
51,ai8hyf/babyagi,https://github.com/ai8hyf/babyagi/tree/6fcd528a92c80846dbb351f2b7babdd50c38709d,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,"babyagi.py/execution_agent()/class SingleTaskListStorage, class DefaultResultsStorage","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available.",['babyagi.py'],['unfriendly user interface']
52,matigumma/bb.agi,https://github.com/matigumma/bb.agi/tree/9af4396c1c97b0de3fc47ceb35ff8e4489be6254,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi.py/task_creation_agent(),"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks.",['babyagi.py'],"['incorrectness', 'slower execution']"
53,matigumma/bb.agi,https://github.com/matigumma/bb.agi/tree/9af4396c1c97b0de3fc47ceb35ff8e4489be6254,incompatible llm output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder.",['babyagi.py'],['incorrectness']
54,matigumma/bb.agi,https://github.com/matigumma/bb.agi/tree/9af4396c1c97b0de3fc47ceb35ff8e4489be6254,unnecessary llm output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi.py/task_creation_agent(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes",['babyagi.py'],"['incorrectness', 'unfriendly user interface']"
55,matigumma/bb.agi,https://github.com/matigumma/bb.agi/tree/9af4396c1c97b0de3fc47ceb35ff8e4489be6254,imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi.py/execution_agent(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py",['babyagi.py'],"['incorrectness', 'slower execution']"
56,matigumma/bb.agi,https://github.com/matigumma/bb.agi/tree/9af4396c1c97b0de3fc47ceb35ff8e4489be6254,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,"babyagi.py/execution_agent()/class SingleTaskListStorage, class DefaultResultsStorage","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available.",['babyagi.py'],['unfriendly user interface']
57,VTSTech/localagi,https://github.com/VTSTech/localagi/tree/85ecc246bd50783e9edaa4227493c0ac127912d8,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi.py/create_task(),"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks.",['babyagi.py'],"['incorrectness', 'slower execution']"
58,VTSTech/localagi,https://github.com/VTSTech/localagi/tree/85ecc246bd50783e9edaa4227493c0ac127912d8,incompatible llm output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi.py/create_task(), update_task()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder.",['babyagi.py'],['incorrectness']
59,VTSTech/localagi,https://github.com/VTSTech/localagi/tree/85ecc246bd50783e9edaa4227493c0ac127912d8,unnecessary llm output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi.py/perform_task(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes",['babyagi.py'],"['incorrectness', 'unfriendly user interface']"
60,VTSTech/localagi,https://github.com/VTSTech/localagi/tree/85ecc246bd50783e9edaa4227493c0ac127912d8,imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi.py/perform_task(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py",['babyagi.py'],"['incorrectness', 'slower execution']"
61,VTSTech/localagi,https://github.com/VTSTech/localagi/tree/85ecc246bd50783e9edaa4227493c0ac127912d8,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,babyagi.py/perform_task(),"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available.",['babyagi.py'],['unfriendly user interface']
62,realminchoi/babyagi-langchain,https://github.com/realminchoi/babyagi-langchain/tree/8173232f8abd09f9b9f8942f0b1cd44327cf66b7,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi.py/TaskCreationChain,"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks.",['babyagi.py'],"['incorrectness', 'slower execution']"
63,realminchoi/babyagi-langchain,https://github.com/realminchoi/babyagi-langchain/tree/8173232f8abd09f9b9f8942f0b1cd44327cf66b7,incompatible llm output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi.py/TaskCreationChain, TaskPrioritizationChain","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder.",['babyagi.py'],['incorrectness']
64,realminchoi/babyagi-langchain,https://github.com/realminchoi/babyagi-langchain/tree/8173232f8abd09f9b9f8942f0b1cd44327cf66b7,unnecessary llm output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi.py/TaskCreationChain,"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes",['babyagi.py'],"['incorrectness', 'unfriendly user interface']"
65,realminchoi/babyagi-langchain,https://github.com/realminchoi/babyagi-langchain/tree/8173232f8abd09f9b9f8942f0b1cd44327cf66b7,imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi.py/execution_chain,"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py",['babyagi.py'],"['incorrectness', 'slower execution']"
66,realminchoi/babyagi-langchain,https://github.com/realminchoi/babyagi-langchain/tree/8173232f8abd09f9b9f8942f0b1cd44327cf66b7,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,babyagi.py/execution_chain,"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available.",['babyagi.py'],['unfriendly user interface']
67,alexdphan/babyagi-chroma-agent,https://github.com/alexdphan/babyagi-chroma-agent/tree/a5379280e526fd559c49d3b85513a339afc302e5,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi-chroma.py/task_creation_chain,"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks.",['babyagi-chroma.py'],"['incorrectness', 'slower execution']"
68,alexdphan/babyagi-chroma-agent,https://github.com/alexdphan/babyagi-chroma-agent/tree/a5379280e526fd559c49d3b85513a339afc302e5,incompatible llm output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi-chroma.py/task_creation_chain,task_prioritization_chain","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder.",['babyagi-chroma.py'],['incorrectness']
69,alexdphan/babyagi-chroma-agent,https://github.com/alexdphan/babyagi-chroma-agent/tree/a5379280e526fd559c49d3b85513a339afc302e5,unnecessary llm output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi-chroma.py/task_creation_chain,"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes",['babyagi-chroma.py'],"['incorrectness', 'unfriendly user interface']"
70,alexdphan/babyagi-chroma-agent,https://github.com/alexdphan/babyagi-chroma-agent/tree/a5379280e526fd559c49d3b85513a339afc302e5,imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi-chroma.py/execution_chain,"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py",['babyagi-chroma.py'],"['incorrectness', 'slower execution']"
71,alexdphan/babyagi-chroma-agent,https://github.com/alexdphan/babyagi-chroma-agent/tree/a5379280e526fd559c49d3b85513a339afc302e5,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,babyagi-chroma.py/execution_chain,"1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available.",['babyagi-chroma.py'],['unfriendly user interface']
72,fendouai/babyagi_zh,https://github.com/fendouai/babyagi_zh/tree/bc907f1d94cd0a6b57db5f6e2b1d0ffbabf6b864,insufficient history management,case1,"Subtasks are sometimes executed repeatedly, often the first task.","IC,SL",babyagi.py/task_creation_agent(),"1.Run the script according to the previously mentioned steps.
2. Observe the tasks and outcoms, until you notice the repeated tasks.",['babyagi.py'],"['incorrectness', 'slower execution']"
73,fendouai/babyagi_zh,https://github.com/fendouai/babyagi_zh/tree/bc907f1d94cd0a6b57db5f6e2b1d0ffbabf6b864,incompatible llm output format,/,Task list numbering continues to reset or get misordered.,IC,"babyagi.py/task_creation_agent(), prioritization_agent()","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder.",['babyagi.py'],['incorrectness']
74,fendouai/babyagi_zh,https://github.com/fendouai/babyagi_zh/tree/bc907f1d94cd0a6b57db5f6e2b1d0ffbabf6b864,unnecessary llm output,/,The task creation process generating too many unnecessary tasks,"IC,UI",babyagi.py/task_creation_agent(),"1.Clone the Repository:
   git clone https://github.com/yoheinakajima/babyagi.git
   cd babyagi
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script:
   python babyagi.py
5.Observe Tasks and Outcomes",['babyagi.py'],"['incorrectness', 'unfriendly user interface']"
75,fendouai/babyagi_zh,https://github.com/fendouai/babyagi_zh/tree/bc907f1d94cd0a6b57db5f6e2b1d0ffbabf6b864,imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL",babyagi.py/execution_agent(),"(steps to use the script:babyagi.py)
1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py",['babyagi.py'],"['incorrectness', 'slower execution']"
76,fendouai/babyagi_zh,https://github.com/fendouai/babyagi_zh/tree/bc907f1d94cd0a6b57db5f6e2b1d0ffbabf6b864,absence of final output,case1,"Tasks and outcomes are only visible on the terminal. Once the instance is closed, all tasks and results are lost. ",UI,"babyagi.py/execution_agent()/class SingleTaskListStorage, class DefaultResultsStorage","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.set the following .env variable:COOPERATIVE_MODE=local
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script: python babyagi.py
10.Close the Instance:Stop the running script by pressing Ctrl+C in the terminal or closing the terminal window.
11.Restart the script to check if the previous tasks and outcomes are retrievable.
12.Notice that the previous tasks and outcomes are not available.",['babyagi.py'],['unfriendly user interface']
77,Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,missing llm input format validation,/,"
UTF8 files unsupported",IC,autogpt/commands/file_operations.py/read_file(),"1.Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT.
(Prompts:
ai_goals:
- Read project-plan-form.htm file 
- Fill project-plan-form.htm for and idea with flying monkeys
ai_name: DocWritter
ai_role: Fill in a doc using a file template)
3.Authorize and wait for the result of the ""read_file ARGUMENTS = {'file': 'project-plan-form.htm'}"" command, which will cause an error: ""Command read_file returned: Error: 'utf-8' codec can't decode byte 0xa0 in position 1341: invalid start byte "".",['autogpt/commands/file_operations.py'],['incorrectness']
78,Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,knowledge misalignment,case1,5. Failing to use pre-seeded data and/or chunk a large JSON file,"ST,IC",AutoGPT/forge/forge/components/context/context.py,"1.(Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.Started a fresh redis server in docker and pre-seeded the issues_data.json (see https://github.com/Significant-Gravitas/AutoGPT/issues/2076) file to redis with:
""python data_ingestion.py --f issues_data.json --init --overlap 300 --max_length 3000""
3.Start Autogpt with: 'python -m autogpt'
  ai_goals:
    - Read design.txt and follow its design specifications.
    - Read advice.txt and obey it every 10 minutes.
    - Use the information saved in your memory to determine the most frequently asked questions from the repos issues posts.
    - Determine the best answer to the most frequently asked questions from the issues comments.
    - Write a FAQ and answer the most frequently asked questions.
  ai_name: GitHubIssuesFAQ-Ai
  ai_role: an AI designed to autonomously manage GitHub issues to make it easier for users to find solutions to their issues.

  advice.txt contains:
    1.Use the data saved in your memory as it already has all the JSON data from the repos you are watching.

4.Used a design.txt file to tell it how it's supposed to work (the content of design.txt see: https://github.com/Significant-Gravitas/AutoGPT/issues/2076)
5.Auto-GPT failed to use pre-seeded data at all and went ahead and downloaded the main repo to /Auto-GPT/auto_gpt_workspace folder to gather data.",['tests/context.py'],"['incorrectness', 'fail-stops']"
79,Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,imprecise knowledge retrieval,/,Add Recency and Importance for Memory Retrieval,"IC,SL","autogpt/agent/agent.py
autogpt/chat.py",The user did not provide specific examples. We can observe AutoGPT's memory performance in its behavior.,"['autogpt/agent/agent.py', 'autogpt/chat.py']","['incorrectness', 'slower execution']"
80,Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/tree/9ef4fab084633e4289226a6dd059a598085ec876,absence of final output,/,"aught in error loop ""No Text to summary""","IC,ST",autogpt/agent/agent.py/start_interaction_loop(self),"1.(MacOS, Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT in continuous mode.
3.Let Auto GPT execute ""browse_website"" command  to collect information from the web about a specific topic. The webpage you directed AutoGPT to browse has no text to summarize or contains other page errors.
4.Observe if AutoGPT enters a loop that after a webpage browsing failure，it restarts the Google search, lands on the same page, and starts the search over, only to land on the page again.",['autogpt/agent/agent.py'],"['incorrectness', 'fail-stops']"
81,FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,insufficient history management,case1,Relevant memory is essentially a duplicate of the conversation（enhancement）,IC,"autogpt/agent/agent.py
autogpt/llm/chat.py","1.Complete the environment setup.
2.(cli mode) start app.
3.Set up a task.
4.Allow app to process the task. Wait for a while, check if app loses track of what it's done in the past and starts to repeat actions.","['autogpt/agent/agent.py', 'autogpt/llm/chat.py']",['incorrectness']
82,FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,missing llm input format validation,/,"
UTF8 files unsupported",IC,autogpt/commands/file_operations.py,"1.Set up and Configure app.
2.Create a Task: Create a task for app that is likely to fail or has complex steps prone to errors.
3.Observe Task Execution:Run the task and let app operate for an extended period. Check if It gets stuck in loops because it thinks failed repeated attempts are relevant and therefore should be tried again and again.
+H266",['autogpt/commands/file_operations.py'],['incorrectness']
83,FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,exceeding llm content limit,case1,"AutoGPT incorrectly interprets the ""429 Too Many Requests"" error as rate limiting, when it is actually due to insufficient API quota from OpenAI's new prepay billing method.",ST,autogpt/llm/providers/openai.py,"1.Complete the environment setup for app.
2.(cli mode) start app in continuous mode.
3.Simulate API Quota Depletion:Ensure that your OpenAI API quota is depleted. This can be done by using up your available credits or not having sufficient pre-paid balance on your OpenAI account.
4.Allow app to make several API requests to OpenAI, which will result in ""429 Too Many Requests"" errors due to the depleted API quota.
",['autogpt/llm/providers/openai.py'],['fail-stops']
84,FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,unnecessary llm output,case1,Failure: command list_files and read_file returned too much output.,"IC,UI","autogpt/llm/chat.py
autogpt/agent/agent.py","1.Configure and launch app according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.5.0)
2.Create an AI agent.
3.Start AutoGPT. Authorize these commands and observe their outputs: 
(1) list_files ARGUMENTS = {'directory': '....\auto_gpt_workspace'} --> SYSTEM: Failure: command list_files returned too much output. Do not execute this command again with the same arguments.
(2)read_file ARGUMENTS = {'filename': '...smoketests_basic_qemu.yml'} --> SYSTEM: Failure: command read_file returned too much output. Do not execute this command again with the same arguments.
","['autogpt/llm/chat.py', 'autogpt/agent/agent.py']","['incorrectness', 'unfriendly user interface']"
85,FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,knowledge misalignment,case1,5. Failing to use pre-seeded data and/or chunk a large JSON file,"ST,IC",tests/context.py,"1.(Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.Started a fresh redis server in docker and pre-seeded the issues_data.json (see https://github.com/Significant-Gravitas/AutoGPT/issues/2076) file to redis with:
""python data_ingestion.py --f issues_data.json --init --overlap 300 --max_length 3000""
3.Start Autogpt with: 'python -m autogpt'
  ai_goals:
    - Read design.txt and follow its design specifications.
    - Read advice.txt and obey it every 10 minutes.
    - Use the information saved in your memory to determine the most frequently asked questions from the repos issues posts.
    - Determine the best answer to the most frequently asked questions from the issues comments.
    - Write a FAQ and answer the most frequently asked questions.
  ai_name: GitHubIssuesFAQ-Ai
  ai_role: an AI designed to autonomously manage GitHub issues to make it easier for users to find solutions to their issues.

  advice.txt contains:
    1.Use the data saved in your memory as it already has all the JSON data from the repos you are watching.

4.Used a design.txt file to tell it how it's supposed to work (the content of design.txt see: https://github.com/Significant-Gravitas/AutoGPT/issues/2076)
5.Auto-GPT failed to use pre-seeded data at all and went ahead and downloaded the main repo to /Auto-GPT/auto_gpt_workspace folder to gather data.",['tests/context.py'],"['incorrectness', 'fail-stops']"
86,FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,conflicting knowledge entries,/,AutoGPT overwrites same text file with each action instead of appending to the end.,"IC,IS","autogpt/agent/agent.py
autogpt/llm/llm_utils.py
autogpt/memory_management/store_memory.py
...","1.Configure and launch app according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.(use RedisMemory)
2.Create an AI agent with a specific role and goals. (Role: an AI agent that specializes in Cisco ACI fabrics and provides expert guidance on the usage of VxLAN technology to enable efficient communication between endpoints in a data center network. Goals: Explain the benefits of VxLAN technology in Cisco ACI fabrics, including increased scalability, flexibility, and network virtualization.)
3.Configure the memory type to RedisMemory.
4.Perform an action that uses the write_to_file command, specifying a file and some text to write.
5.Perform an action that uses the append_to_file command, specifying the same file and additional text to append.
6. Check the contents of the file after each action to see if the text is being overwritten or appended correctly.","['autogpt/agent/agent.py', 'autogpt/llm/llm_utils.py', 'autogpt/memory_management/store_memory.py']","['incorrectness', 'insecure']"
87,FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,imprecise knowledge retrieval,/,Add Recency and Importance for Memory Retrieval,"IC,SL","autogpt/agent/agent.py
autogpt/chat.py",The user did not provide specific examples. We can observe app's memory performance in its behavior.,"['autogpt/agent/agent.py', 'autogpt/chat.py']","['incorrectness', 'slower execution']"
88,FOLLGAD/Godmode-GPT,https://github.com/FOLLGAD/Godmode-GPT/tree/4e1dc7055580ab9bb3ce85114f9e17950ac96d7b,absence of final output,/,"aught in error loop ""No Text to summary""","IC,ST",autogpt/agent/agent.py/start_interaction_loop(self),"1.(MacOS, Set up with Docker)Complete the environment setup for app according to the instructions on v0.2.2 READMe.md
2. Run app in continuous mode.
3.Let app execute ""browse_website"" command  to collect information from the web about a specific topic. The webpage you directed AutoGPT to browse has no text to summarize or contains other page errors.
4.Observe if app enters a loop that after a webpage browsing failure，it restarts the Google search, lands on the same page, and starts the search over, only to land on the page again.",['autogpt/agent/agent.py'],"['incorrectness', 'fail-stops']"
89,BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,insufficient history management,case1,Relevant memory is essentially a duplicate of the conversation（enhancement）,IC,"autogpt/agent/agent.py
autogpt/chat.py","1.Complete the environment setup for app according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task.
4.Allow AutoGPT to process the task. Wait for a while, check if Auto-GPT loses track of what it's done in the past and starts to repeat actions.","['autogpt/agent/agent.py', 'autogpt/chat.py']",['incorrectness']
90,BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,missing llm input format validation,/,"
UTF8 files unsupported",IC,autogpt/commands/file_operations.py/read_file(),"1.Complete the environment setup for app according to the instructions on v0.2.2 READMe.md
2. Run app.
(Prompts:
ai_goals:
- Read project-plan-form.htm file 
- Fill project-plan-form.htm for and idea with flying monkeys
ai_name: DocWritter
ai_role: Fill in a doc using a file template)
3.Authorize and wait for the result of the ""read_file ARGUMENTS = {'file': 'project-plan-form.htm'}"" command, which will cause an error: ""Command read_file returned: Error: 'utf-8' codec can't decode byte 0xa0 in position 1341: invalid start byte "".",['autogpt/commands/file_operations.py'],['incorrectness']
91,BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,knowledge misalignment,case1,5. Failing to use pre-seeded data and/or chunk a large JSON file,"ST,IC",AutoGPT/forge/forge/components/context/context.py,"1.(Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.Started a fresh redis server in docker and pre-seeded the issues_data.json (see https://github.com/Significant-Gravitas/AutoGPT/issues/2076) file to redis with:
""python data_ingestion.py --f issues_data.json --init --overlap 300 --max_length 3000""
3.Start Autogpt with: 'python -m autogpt'
  ai_goals:
    - Read design.txt and follow its design specifications.
    - Read advice.txt and obey it every 10 minutes.
    - Use the information saved in your memory to determine the most frequently asked questions from the repos issues posts.
    - Determine the best answer to the most frequently asked questions from the issues comments.
    - Write a FAQ and answer the most frequently asked questions.
  ai_name: GitHubIssuesFAQ-Ai
  ai_role: an AI designed to autonomously manage GitHub issues to make it easier for users to find solutions to their issues.

  advice.txt contains:
    1.Use the data saved in your memory as it already has all the JSON data from the repos you are watching.

4.Used a design.txt file to tell it how it's supposed to work (the content of design.txt see: https://github.com/Significant-Gravitas/AutoGPT/issues/2076)
5.Auto-GPT failed to use pre-seeded data at all and went ahead and downloaded the main repo to /Auto-GPT/auto_gpt_workspace folder to gather data.",['tests/context.py'],"['incorrectness', 'fail-stops']"
92,BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,imprecise knowledge retrieval,/,Add Recency and Importance for Memory Retrieval,"IC,SL","autogpt/agent/agent.py
autogpt/chat.py",The user did not provide specific examples. We can observe AutoGPT's memory performance in its behavior.,"['autogpt/agent/agent.py', 'autogpt/chat.py']","['incorrectness', 'slower execution']"
93,BillSchumacher/Auto-GPT,https://github.com/BillSchumacher/Auto-GPT/tree/bdd07b18bea674cf756ebfb3a0a8915042d9126f,absence of final output,/,"aught in error loop ""No Text to summary""","IC,ST",autogpt/agent/agent.py/start_interaction_loop(self),"1.(MacOS, Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT in continuous mode.
3.Let Auto GPT execute ""browse_website"" command  to collect information from the web about a specific topic. The webpage you directed AutoGPT to browse has no text to summarize or contains other page errors.
4.Observe if AutoGPT enters a loop that after a webpage browsing failure，it restarts the Google search, lands on the same page, and starts the search over, only to land on the page again.",['autogpt/agent/agent.py'],"['incorrectness', 'fail-stops']"
94,amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,insufficient history management,case1,Relevant memory is essentially a duplicate of the conversation（enhancement）,IC,"autogpt/agent/agent.py
autogpt/chat.py","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task.
4.Allow AutoGPT to process the task. Wait for a while, check if Auto-GPT loses track of what it's done in the past and starts to repeat actions.","['autogpt/agent/agent.py', 'autogpt/chat.py']",['incorrectness']
95,amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,missing llm input format validation,/,"
UTF8 files unsupported",IC,autogpt/commands/file_operations.py/read_file(),"1.Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT.
(Prompts:
ai_goals:
- Read project-plan-form.htm file 
- Fill project-plan-form.htm for and idea with flying monkeys
ai_name: DocWritter
ai_role: Fill in a doc using a file template)
3.Authorize and wait for the result of the ""read_file ARGUMENTS = {'file': 'project-plan-form.htm'}"" command, which will cause an error: ""Command read_file returned: Error: 'utf-8' codec can't decode byte 0xa0 in position 1341: invalid start byte "".",['autogpt/commands/file_operations.py'],['incorrectness']
96,amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,knowledge misalignment,case1,Failing to use pre-seeded data and/or chunk a large JSON file,"ST,IC",AutoGPT/forge/forge/components/context/context.py,"1.(Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.Started a fresh redis server in docker and pre-seeded the issues_data.json (see https://github.com/Significant-Gravitas/AutoGPT/issues/2076) file to redis with:
""python data_ingestion.py --f issues_data.json --init --overlap 300 --max_length 3000""
3.Start Autogpt with: 'python -m autogpt'
  ai_goals:
    - Read design.txt and follow its design specifications.
    - Read advice.txt and obey it every 10 minutes.
    - Use the information saved in your memory to determine the most frequently asked questions from the repos issues posts.
    - Determine the best answer to the most frequently asked questions from the issues comments.
    - Write a FAQ and answer the most frequently asked questions.
  ai_name: GitHubIssuesFAQ-Ai
  ai_role: an AI designed to autonomously manage GitHub issues to make it easier for users to find solutions to their issues.

  advice.txt contains:
    1.Use the data saved in your memory as it already has all the JSON data from the repos you are watching.

4.Used a design.txt file to tell it how it's supposed to work (the content of design.txt see: https://github.com/Significant-Gravitas/AutoGPT/issues/2076)
5.Auto-GPT failed to use pre-seeded data at all and went ahead and downloaded the main repo to /Auto-GPT/auto_gpt_workspace folder to gather data.",['tests/context.py'],"['incorrectness', 'fail-stops']"
97,amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,imprecise knowledge retrieval,/,Add Recency and Importance for Memory Retrieval,"IC,SL","autogpt/agent/agent.py
autogpt/chat.py",The user did not provide specific examples. We can observe AutoGPT's memory performance in its behavior.,"['autogpt/agent/agent.py', 'autogpt/chat.py']","['incorrectness', 'slower execution']"
98,amauryfischer/Auto-GPT-WebUI,https://github.com/amauryfischer/Auto-GPT-WebUI/tree/cea68df18f309b094912bdaa6ce2cd484dca5a13,absence of final output,/,"aught in error loop ""No Text to summary""","IC,ST",autogpt/agent/agent.py/start_interaction_loop(self),"1.(MacOS, Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT in continuous mode.
3.Let Auto GPT execute ""browse_website"" command  to collect information from the web about a specific topic. The webpage you directed AutoGPT to browse has no text to summarize or contains other page errors.
4.Observe if AutoGPT enters a loop that after a webpage browsing failure，it restarts the Google search, lands on the same page, and starts the search over, only to land on the page again.",['autogpt/agent/agent.py'],"['incorrectness', 'fail-stops']"
99,ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,insufficient history management,case1,Relevant memory is essentially a duplicate of the conversation（enhancement）,IC,autogpts/forge/forge/agent.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task.
4.Allow AutoGPT to process the task. Wait for a while, check if Auto-GPT loses track of what it's done in the past and starts to repeat actions.",['autogpts/forge/forge/agent.py'],['incorrectness']
100,ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,missing llm input format validation,/,"
UTF8 files unsupported",IC,autogpts/autogpt/autogpt/commands/file_operations_utils.py,"1.Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT.
(Prompts:
ai_goals:
- Read project-plan-form.htm file 
- Fill project-plan-form.htm for and idea with flying monkeys
ai_name: DocWritter
ai_role: Fill in a doc using a file template)
3.Authorize and wait for the result of the ""read_file ARGUMENTS = {'file': 'project-plan-form.htm'}"" command, which will cause an error: ""Command read_file returned: Error: 'utf-8' codec can't decode byte 0xa0 in position 1341: invalid start byte "".",['autogpts/autogpt/autogpt/commands/file_operations_utils.py'],['incorrectness']
101,ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,exceeding llm content limit,case1,"AutoGPT incorrectly interprets the ""429 Too Many Requests"" error as rate limiting, when it is actually due to insufficient API quota from OpenAI's new prepay billing method.",ST,autogpts/autogpt/autogpt/llm/providers/openai.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh --continuous"" to start AutoGPT in continuous mode.
3.Simulate API Quota Depletion:Ensure that your OpenAI API quota is depleted. This can be done by using up your available credits or not having sufficient pre-paid balance on your OpenAI account.
4.Allow AutoGPT to make several API requests to OpenAI, which will result in ""429 Too Many Requests"" errors due to the depleted API quota.
",['autogpts/autogpt/autogpt/llm/providers/openai.py'],['fail-stops']
102,ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,unnecessary llm output,case1,Failure: command list_files and read_file returned too much output.,"IC,UI",autogpts/forge/forge/agent.py,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.5.0)
2.Create an AI agent.
3.Start AutoGPT. Authorize these commands and observe their outputs: 
(1) list_files ARGUMENTS = {'directory': '....\auto_gpt_workspace'} --> SYSTEM: Failure: command list_files returned too much output. Do not execute this command again with the same arguments.
(2)read_file ARGUMENTS = {'filename': '...smoketests_basic_qemu.yml'} --> SYSTEM: Failure: command read_file returned too much output. Do not execute this command again with the same arguments.
",['autogpts/forge/forge/agent.py'],"['incorrectness', 'unfriendly user interface']"
103,ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,knowledge misalignment,case1,5. Failing to use pre-seeded data and/or chunk a large JSON file,"ST,IC",autogpts/autogpt/autogpt/agents/features/context.py,"1.(Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.Started a fresh redis server in docker and pre-seeded the issues_data.json (see https://github.com/Significant-Gravitas/AutoGPT/issues/2076) file to redis with:
""python data_ingestion.py --f issues_data.json --init --overlap 300 --max_length 3000""
3.Start Autogpt with: 'python -m autogpt'
  ai_goals:
    - Read design.txt and follow its design specifications.
    - Read advice.txt and obey it every 10 minutes.
    - Use the information saved in your memory to determine the most frequently asked questions from the repos issues posts.
    - Determine the best answer to the most frequently asked questions from the issues comments.
    - Write a FAQ and answer the most frequently asked questions.
  ai_name: GitHubIssuesFAQ-Ai
  ai_role: an AI designed to autonomously manage GitHub issues to make it easier for users to find solutions to their issues.

  advice.txt contains:
    1.Use the data saved in your memory as it already has all the JSON data from the repos you are watching.

4.Used a design.txt file to tell it how it's supposed to work (the content of design.txt see: https://github.com/Significant-Gravitas/AutoGPT/issues/2076)
5.Auto-GPT failed to use pre-seeded data at all and went ahead and downloaded the main repo to /Auto-GPT/auto_gpt_workspace folder to gather data.",['autogpts/autogpt/autogpt/agents/features/context.py'],"['incorrectness', 'fail-stops']"
104,ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,conflicting knowledge entries,/,AutoGPT overwrites same text file with each action instead of appending to the end.,IC,"autogpts/forge/forge/agent.py
","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.(use RedisMemory)
2.Create an AI agent with a specific role and goals. (Role: an AI agent that specializes in Cisco ACI fabrics and provides expert guidance on the usage of VxLAN technology to enable efficient communication between endpoints in a data center network. Goals: Explain the benefits of VxLAN technology in Cisco ACI fabrics, including increased scalability, flexibility, and network virtualization.)
3.Configure the memory type to RedisMemory.
4.Perform an action that uses the write_to_file command, specifying a file and some text to write.
5.Perform an action that uses the append_to_file command, specifying the same file and additional text to append.
6. Check the contents of the file after each action to see if the text is being overwritten or appended correctly.",['autogpts/forge/forge/agent.py'],['incorrectness']
105,ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,imprecise knowledge retrieval,/,Add Recency and Importance for Memory Retrieval,"IC,SL",autogpts/forge/forge/agent.py,The user did not provide specific examples. We can observe AutoGPT's memory performance in its behavior.,['autogpts/forge/forge/agent.py'],"['incorrectness', 'slower execution']"
106,ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,absence of final output,/,"aught in error loop ""No Text to summary""","IC,ST",autogpts/forge/forge/agent.py,"1.(MacOS, Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT in continuous mode.
3.Let Auto GPT execute ""browse_website"" command  to collect information from the web about a specific topic. The webpage you directed AutoGPT to browse has no text to summarize or contains other page errors.
4.Observe if AutoGPT enters a loop that after a webpage browsing failure，it restarts the Google search, lands on the same page, and starts the search over, only to land on the page again.",['autogpts/forge/forge/agent.py'],"['incorrectness', 'fail-stops']"
107,ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,low-frequency interactivity,/,"
If you do not interact with Auto-GPT frequently enough you will get an API error when you resume.","IC,UI",autogpts/autogpt/autogpt/llm/providers/openai.py,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.In manual mode, do not interact with Auto-GPT for ""a long time"" (3 hours for example).
3.Try to interact with Auto-GPT again and observe if you encounter any API errors or connection timeouts.",['autogpts/autogpt/autogpt/llm/providers/openai.py'],"['incorrectness', 'unfriendly user interface']"
108,ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,privacy violation,/,"By default, AutoGPT can access the user's browser.",IS,autogpts/autogpt/autogpt/commands/web_search.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Use default Settings)
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task, which involves web browsing and searching.
4.Allow AutoGPT to process the task and check if AutoGPT accesses the browser without user consent.",['autogpts/autogpt/autogpt/commands/web_search.py'],['insecure']
109,ATheorell/AutoGPTArenaHack,https://github.com/ATheorell/AutoGPTArenaHack/tree/1e4f2dc004b92b9f236543674f94fb9f0af9bb2e,inefficient memory management,/,Memory Feature seems to not work on gpt3only mode,IC,autogpts/autogpt/autogpt/agents/agent.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Set API Keys for OpenAI, Pinecone, and Eleven Labs, then set the smart model to gpt-3.5-turbo)
2.(cli mode)Use the command: ""call python ./scripts/main.py --gpt3only --debug"" to start AutoGPT with the --gpt3only option.
3.Prompt GPT to be a developer, and give it the goal to load a design document text file into long term memory. and the additional goals to develop the project.
(Prompt: 
ai_goals:
- Load the design document file under `./designdocument.txt` into long term memory.
- Find out the currently created files in the unity project under `./UnityProject/Assets`,
  then improve the code according to the design document, solving any issues or bugs
  found.
- Find out what needs to be implemented next, then implement it, then do this step
  again until there is nothing left to implement.
- Double check and make sure everything is implemented in the Unity Project Files
  from your understanding of the project, then refine and criticize the code making
  sure everything is in working order.
- Shut Down.
ai_name: Unity-Dev-GPT
ai_role: An AI designed to build Video Games, Libraries, Editor Extensions, and Packages for Unity LTS 2021.)
4.Allow AutoGPT to process the task and check for any JSON format errors or issues with memory handling.",['autogpts/autogpt/autogpt/agents/agent.py'],['incorrectness']
110,catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,insufficient history management,case1,Relevant memory is essentially a duplicate of the conversation（enhancement）,IC,"scripts/agent_manager.py
scripts/chat.py","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task.
4.Allow AutoGPT to process the task. Wait for a while, check if Auto-GPT loses track of what it's done in the past and starts to repeat actions.","['scripts/agent_manager.py', 'scripts/chat.py']",['incorrectness']
111,catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,missing llm input format validation,/,"
UTF8 files unsupported",IC,scripts/file_operations.py/read_file(),"1.Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT.
(Prompts:
ai_goals:
- Read project-plan-form.htm file 
- Fill project-plan-form.htm for and idea with flying monkeys
ai_name: DocWritter
ai_role: Fill in a doc using a file template)
3.Authorize and wait for the result of the ""read_file ARGUMENTS = {'file': 'project-plan-form.htm'}"" command, which will cause an error: ""Command read_file returned: Error: 'utf-8' codec can't decode byte 0xa0 in position 1341: invalid start byte "".",['scripts/file_operations.py'],['incorrectness']
112,catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,exceeding llm content limit,case1,"AutoGPT incorrectly interprets the ""429 Too Many Requests"" error as rate limiting, when it is actually due to insufficient API quota from OpenAI's new prepay billing method.",ST,scripts/config.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh --continuous"" to start AutoGPT in continuous mode.
3.Simulate API Quota Depletion:Ensure that your OpenAI API quota is depleted. This can be done by using up your available credits or not having sufficient pre-paid balance on your OpenAI account.
4.Allow AutoGPT to make several API requests to OpenAI, which will result in ""429 Too Many Requests"" errors due to the depleted API quota.
",['scripts/config.py'],['fail-stops']
113,catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,unnecessary llm output,case1,Failure: command list_files and read_file returned too much output.,"IC,UI","scripts/chat.py
scripts/agent_manager.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.5.0)
2.Create an AI agent.
3.Start AutoGPT. Authorize these commands and observe their outputs: 
(1) list_files ARGUMENTS = {'directory': '....\auto_gpt_workspace'} --> SYSTEM: Failure: command list_files returned too much output. Do not execute this command again with the same arguments.
(2)read_file ARGUMENTS = {'filename': '...smoketests_basic_qemu.yml'} --> SYSTEM: Failure: command read_file returned too much output. Do not execute this command again with the same arguments.
","['scripts/agent_manager.py', 'scripts/chat.py']","['incorrectness', 'unfriendly user interface']"
114,catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,knowledge misalignment,case1,5. Failing to use pre-seeded data and/or chunk a large JSON file,"ST,IC",scripts/chat.py,"1.(Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.Started a fresh redis server in docker and pre-seeded the issues_data.json (see https://github.com/Significant-Gravitas/AutoGPT/issues/2076) file to redis with:
""python data_ingestion.py --f issues_data.json --init --overlap 300 --max_length 3000""
3.Start Autogpt with: 'python -m autogpt'
  ai_goals:
    - Read design.txt and follow its design specifications.
    - Read advice.txt and obey it every 10 minutes.
    - Use the information saved in your memory to determine the most frequently asked questions from the repos issues posts.
    - Determine the best answer to the most frequently asked questions from the issues comments.
    - Write a FAQ and answer the most frequently asked questions.
  ai_name: GitHubIssuesFAQ-Ai
  ai_role: an AI designed to autonomously manage GitHub issues to make it easier for users to find solutions to their issues.

  advice.txt contains:
    1.Use the data saved in your memory as it already has all the JSON data from the repos you are watching.

4.Used a design.txt file to tell it how it's supposed to work (the content of design.txt see: https://github.com/Significant-Gravitas/AutoGPT/issues/2076)
5.Auto-GPT failed to use pre-seeded data at all and went ahead and downloaded the main repo to /Auto-GPT/auto_gpt_workspace folder to gather data.",['scripts/chat.py'],"['incorrectness', 'fail-stops']"
115,catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,conflicting knowledge entries,/,AutoGPT overwrites same text file with each action instead of appending to the end.,"IC,IS","scripts/agent_manager.py
scripts/memory.py
scripts/commands.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.(use RedisMemory)
2.Create an AI agent with a specific role and goals. (Role: an AI agent that specializes in Cisco ACI fabrics and provides expert guidance on the usage of VxLAN technology to enable efficient communication between endpoints in a data center network. Goals: Explain the benefits of VxLAN technology in Cisco ACI fabrics, including increased scalability, flexibility, and network virtualization.)
3.Configure the memory type to RedisMemory.
4.Perform an action that uses the write_to_file command, specifying a file and some text to write.
5.Perform an action that uses the append_to_file command, specifying the same file and additional text to append.
6. Check the contents of the file after each action to see if the text is being overwritten or appended correctly.","['scripts/commands.py', 'scripts/agent_manager.py']","['incorrectness', 'insecure']"
116,catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,imprecise knowledge retrieval,/,Add Recency and Importance for Memory Retrieval,"IC,SL","scripts/agent_manager.py
scripts/chat.py",The user did not provide specific examples. We can observe AutoGPT's memory performance in its behavior.,"['scripts/agent_manager.py', 'scripts/chat.py']","['incorrectness', 'slower execution']"
117,catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,absence of final output,/,"aught in error loop ""No Text to summary""","IC,ST",scripts/agent_manager.py,"1.(MacOS, Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT in continuous mode.
3.Let Auto GPT execute ""browse_website"" command  to collect information from the web about a specific topic. The webpage you directed AutoGPT to browse has no text to summarize or contains other page errors.
4.Observe if AutoGPT enters a loop that after a webpage browsing failure，it restarts the Google search, lands on the same page, and starts the search over, only to land on the page again.",['scripts/agent_manager.py'],"['incorrectness', 'fail-stops']"
118,catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,low-frequency interactivity,/,"
If you do not interact with Auto-GPT frequently enough you will get an API error when you resume.","IC,UI",scripts/config.py,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.In manual mode, do not interact with Auto-GPT for ""a long time"" (3 hours for example).
3.Try to interact with Auto-GPT again and observe if you encounter any API errors or connection timeouts.",['scripts/config.py'],"['incorrectness', 'unfriendly user interface']"
119,catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,privacy violation,/,"By default, AutoGPT can access the user's browser.",IS,scripts/commands.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Use default Settings)
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task, which involves web browsing and searching.
4.Allow AutoGPT to process the task and check if AutoGPT accesses the browser without user consent.",['scripts/commands.py'],['insecure']
120,catid/Supercharger-Auto-GPT,https://github.com/catid/Supercharger-Auto-GPT/tree/2f8066fb4121ba0f724cbe92c4cbef91ecf5cc29,inefficient memory management,/,Memory Feature seems to not work on gpt3only mode,IC,"scripts/config.py
scripts/agent_manager.py","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Set API Keys for OpenAI, Pinecone, and Eleven Labs, then set the smart model to gpt-3.5-turbo)
2.(cli mode)Use the command: ""call python ./scripts/main.py --gpt3only --debug"" to start AutoGPT with the --gpt3only option.
3.Prompt GPT to be a developer, and give it the goal to load a design document text file into long term memory. and the additional goals to develop the project.
(Prompt: 
ai_goals:
- Load the design document file under `./designdocument.txt` into long term memory.
- Find out the currently created files in the unity project under `./UnityProject/Assets`,
  then improve the code according to the design document, solving any issues or bugs
  found.
- Find out what needs to be implemented next, then implement it, then do this step
  again until there is nothing left to implement.
- Double check and make sure everything is implemented in the Unity Project Files
  from your understanding of the project, then refine and criticize the code making
  sure everything is in working order.
- Shut Down.
ai_name: Unity-Dev-GPT
ai_role: An AI designed to build Video Games, Libraries, Editor Extensions, and Packages for Unity LTS 2021.)
4.Allow AutoGPT to process the task and check for any JSON format errors or issues with memory handling.","['scripts/agent_manager.py', 'scripts/config.py']",['incorrectness']
121,edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,insufficient history management,case1,Relevant memory is essentially a duplicate of the conversation（enhancement）,IC,"scripts/agent_manager.py
scripts/chat.py","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task.
4.Allow AutoGPT to process the task. Wait for a while, check if Auto-GPT loses track of what it's done in the past and starts to repeat actions.","['scripts/agent_manager.py', 'scripts/chat.py']",['incorrectness']
122,edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,missing llm input format validation,/,"
UTF8 files unsupported",IC,scripts/file_operations.py/read_file(),"1.Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT.
(Prompts:
ai_goals:
- Read project-plan-form.htm file 
- Fill project-plan-form.htm for and idea with flying monkeys
ai_name: DocWritter
ai_role: Fill in a doc using a file template)
3.Authorize and wait for the result of the ""read_file ARGUMENTS = {'file': 'project-plan-form.htm'}"" command, which will cause an error: ""Command read_file returned: Error: 'utf-8' codec can't decode byte 0xa0 in position 1341: invalid start byte "".",['scripts/file_operations.py'],['incorrectness']
123,edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,exceeding llm content limit,case1,"AutoGPT incorrectly interprets the ""429 Too Many Requests"" error as rate limiting, when it is actually due to insufficient API quota from OpenAI's new prepay billing method.",ST,scripts/config.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.(cli mode)Use the command: ""./autogpt.sh --continuous"" to start AutoGPT in continuous mode.
3.Simulate API Quota Depletion:Ensure that your OpenAI API quota is depleted. This can be done by using up your available credits or not having sufficient pre-paid balance on your OpenAI account.
4.Allow AutoGPT to make several API requests to OpenAI, which will result in ""429 Too Many Requests"" errors due to the depleted API quota.
",['scripts/config.py'],['fail-stops']
124,edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,unnecessary llm output,case1,Failure: command list_files and read_file returned too much output.,"IC,UI","scripts/chat.py
scripts/agent_manager.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/.(Use releases before v0.5.0)
2.Create an AI agent.
3.Start AutoGPT. Authorize these commands and observe their outputs: 
(1) list_files ARGUMENTS = {'directory': '....\auto_gpt_workspace'} --> SYSTEM: Failure: command list_files returned too much output. Do not execute this command again with the same arguments.
(2)read_file ARGUMENTS = {'filename': '...smoketests_basic_qemu.yml'} --> SYSTEM: Failure: command read_file returned too much output. Do not execute this command again with the same arguments.
","['scripts/agent_manager.py', 'scripts/chat.py']","['incorrectness', 'unfriendly user interface']"
125,edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,knowledge misalignment,case1,Failing to use pre-seeded data and/or chunk a large JSON file,"ST,IC",scripts/chat.py,"1.(Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.Started a fresh redis server in docker and pre-seeded the issues_data.json (see https://github.com/Significant-Gravitas/AutoGPT/issues/2076) file to redis with:
""python data_ingestion.py --f issues_data.json --init --overlap 300 --max_length 3000""
3.Start Autogpt with: 'python -m autogpt'
  ai_goals:
    - Read design.txt and follow its design specifications.
    - Read advice.txt and obey it every 10 minutes.
    - Use the information saved in your memory to determine the most frequently asked questions from the repos issues posts.
    - Determine the best answer to the most frequently asked questions from the issues comments.
    - Write a FAQ and answer the most frequently asked questions.
  ai_name: GitHubIssuesFAQ-Ai
  ai_role: an AI designed to autonomously manage GitHub issues to make it easier for users to find solutions to their issues.

  advice.txt contains:
    1.Use the data saved in your memory as it already has all the JSON data from the repos you are watching.

4.Used a design.txt file to tell it how it's supposed to work (the content of design.txt see: https://github.com/Significant-Gravitas/AutoGPT/issues/2076)
5.Auto-GPT failed to use pre-seeded data at all and went ahead and downloaded the main repo to /Auto-GPT/auto_gpt_workspace folder to gather data.",['scripts/chat.py'],"['incorrectness', 'fail-stops']"
126,edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,conflicting knowledge entries,/,AutoGPT overwrites same text file with each action instead of appending to the end.,IC,"scripts/agent_manager.py
scripts/memory.py
scripts/commands.py","1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.(use RedisMemory)
2.Create an AI agent with a specific role and goals. (Role: an AI agent that specializes in Cisco ACI fabrics and provides expert guidance on the usage of VxLAN technology to enable efficient communication between endpoints in a data center network. Goals: Explain the benefits of VxLAN technology in Cisco ACI fabrics, including increased scalability, flexibility, and network virtualization.)
3.Configure the memory type to RedisMemory.
4.Perform an action that uses the write_to_file command, specifying a file and some text to write.
5.Perform an action that uses the append_to_file command, specifying the same file and additional text to append.
6. Check the contents of the file after each action to see if the text is being overwritten or appended correctly.","['scripts/commands.py', 'scripts/agent_manager.py']",['incorrectness']
127,edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,imprecise knowledge retrieval,/,Add Recency and Importance for Memory Retrieval,"IC,SL","scripts/agent_manager.py
scripts/chat.py",The user did not provide specific examples. We can observe AutoGPT's memory performance in its behavior.,"['scripts/agent_manager.py', 'scripts/chat.py']","['incorrectness', 'slower execution']"
128,edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,absence of final output,/,"aught in error loop ""No Text to summary""","IC,ST",scripts/agent_manager.py,"1.(MacOS, Set up with Docker)Complete the environment setup for AutoGPT according to the instructions on v0.2.2 READMe.md
2. Run AutoGPT in continuous mode.
3.Let Auto GPT execute ""browse_website"" command  to collect information from the web about a specific topic. The webpage you directed AutoGPT to browse has no text to summarize or contains other page errors.
4.Observe if AutoGPT enters a loop that after a webpage browsing failure，it restarts the Google search, lands on the same page, and starts the search over, only to land on the page again.",['scripts/agent_manager.py'],"['incorrectness', 'fail-stops']"
129,edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,low-frequency interactivity,/,"
If you do not interact with Auto-GPT frequently enough you will get an API error when you resume.","IC,UI",scripts/config.py,"1.Configure and launch Auto-GPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
2.In manual mode, do not interact with Auto-GPT for ""a long time"" (3 hours for example).
3.Try to interact with Auto-GPT again and observe if you encounter any API errors or connection timeouts.",['scripts/config.py'],"['incorrectness', 'unfriendly user interface']"
130,edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,privacy violation,/,"By default, AutoGPT can access the user's browser.",IS,scripts/commands.py,"1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Use default Settings)
2.(cli mode)Use the command: ""./autogpt.sh run --help "" to start AutoGPT.
3.Set up a task, which involves web browsing and searching.
4.Allow AutoGPT to process the task and check if AutoGPT accesses the browser without user consent.",['scripts/commands.py'],['insecure']
131,edik7333/Auto-GPT-llama-cpp,https://github.com/edik7333/Auto-GPT-llama-cpp/tree/2b4bf189825c0abe28bf6f020b87921d82636d67,inefficient memory management,/,Memory Feature seems to not work on gpt3only mode,IC,"scripts/config.py
scripts/agent_manager.py","1.Complete the environment setup for AutoGPT according to the instructions on this website: https://docs.agpt.co/autogpt/setup/.
(Set API Keys for OpenAI, Pinecone, and Eleven Labs, then set the smart model to gpt-3.5-turbo)
2.(cli mode)Use the command: ""call python ./scripts/main.py --gpt3only --debug"" to start AutoGPT with the --gpt3only option.
3.Prompt GPT to be a developer, and give it the goal to load a design document text file into long term memory. and the additional goals to develop the project.
(Prompt: 
ai_goals:
- Load the design document file under `./designdocument.txt` into long term memory.
- Find out the currently created files in the unity project under `./UnityProject/Assets`,
  then improve the code according to the design document, solving any issues or bugs
  found.
- Find out what needs to be implemented next, then implement it, then do this step
  again until there is nothing left to implement.
- Double check and make sure everything is implemented in the Unity Project Files
  from your understanding of the project, then refine and criticize the code making
  sure everything is in working order.
- Shut Down.
ai_name: Unity-Dev-GPT
ai_role: An AI designed to build Video Games, Libraries, Editor Extensions, and Packages for Unity LTS 2021.)
4.Allow AutoGPT to process the task and check for any JSON format errors or issues with memory handling.","['scripts/agent_manager.py', 'scripts/config.py']",['incorrectness']
132,hlohaus/Agent-LLM,https://github.com/hlohaus/Agent-LLM/tree/d8a46dacea35c6224effd3e733d5c085bea1949a,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,CustomPrompt.py,"1.In the UI, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""",['CustomPrompt.py'],['incorrectness']
133,hlohaus/Agent-LLM,https://github.com/hlohaus/Agent-LLM/tree/d8a46dacea35c6224effd3e733d5c085bea1949a,missing llm input format validation,/,TypeError: Object of type bytes is not JSON serializable(pdf),IC,app.py,"click ""Interact"", choose ""Learning"", choose agent, method from file, choose a pdf and upload",['app.py'],['incorrectness']
134,hlohaus/Agent-LLM,https://github.com/hlohaus/Agent-LLM/tree/d8a46dacea35c6224effd3e733d5c085bea1949a,insufficient history management,/,Stuck creating memories ,CL,AgentLLM.py,"1.streamlit run Main.py

2.Run textgen with: python server.py --listen-port 7861 --wbits 4 --groupsize 128 --model llama-13b-4bit-128g --model_type llama --xformers --api
See that it is running:
Starting streaming server at ws://127.0.0.1:5005/api/v1/stream Starting API at http://127.0.0.1:5000/api Running on local URL:  http://127.0.0.1:7861

3.Create new agent with config:

{""commands"": {""Read Audio from File"": false, ""Read Audio"": false, ""Evaluate Code"": true, ""Analyze Pull Request"": false, ""Perform Automated Testing"": false, ""Run CI-CD Pipeline"": false, ""Improve Code"": false, ""Write Tests"": false, ""Create a new command"": false, ""Execute Python File"": false, ""Execute Shell"": false, ""Read File"": true, ""Write to File"": true, ""Append to File"": true, ""Delete File"": true, ""Search Files"": true, ""Clone Github Repository"": false, ""Google Search"": true, ""Searx Search"": false, ""Get Datetime"": false, ""Speak with TTS"": false, ""Scrape Text with Playwright"": false, ""Scrape Links with Playwright"": false, ""Is Valid URL"": false, ""Sanitize URL"": false, ""Check Local File Access"": false, ""Get Response"": true, ""Scrape Text"": true, ""Scrape Links"": true, ""Scrape Text with Selenium"": false, ""Ask AI Agent gpt4free"": false, ""Instruct AI Agent gpt4free"": false, ""Ask AI Agent Ooga"": false, ""Instruct AI Agent Ooga"": false}, ""settings"": {""provider"": ""oobabooga"", ""AI_MODEL"": ""gpt-3.5-turbo"", ""AI_TEMPERATURE"": ""0.4"", ""MAX_TOKENS"": ""4000"", ""embedder"": ""default"", ""AI_PROVIDER_URI"": ""http://127.0.0.1:5000"", ""HUGGINGFACE_AUDIO_TO_TEXT_MODEL"": ""facebook/wav2vec2-large-960h-lv60-self"", ""DISCORD_COMMAND_PREFIX"": ""/AGiXT"", ""WORKING_DIRECTORY"": ""./WORKSPACE"", ""WORKING_DIRECTORY_RESTRICTED"": ""True"", ""SEARXNG_INSTANCE_URL"": ""https://searx.work"", ""USE_BRIAN_TTS"": ""True"", ""ELEVENLABS_VOICE"": ""Josh"", ""SELENIUM_WEB_BROWSER"": ""chrome"", """": """"}}

4.Ask it to do anything or chat with it.
5.Behavior: Agent is unable to do anything, program is stuck trying to create memories.",['AgentLLM.py'],['slower execution']
135,hlohaus/Agent-LLM,https://github.com/hlohaus/Agent-LLM/tree/d8a46dacea35c6224effd3e733d5c085bea1949a,imprecise knowledge retrieval,/,Unable to retrieve data ,"CL,SL",commands/google.py,"1.Setup with docker.  (Local)
2.Run Gen and run Task Chain.",['commands/google.py'],['slower execution']
136,hlohaus/Agent-LLM,https://github.com/hlohaus/Agent-LLM/tree/d8a46dacea35c6224effd3e733d5c085bea1949a,resource contention,/,Playwright Sync API inside the asyncio loop error,ST,commands/web_playwright.py,"(Environment Type - Connection: Local;  Environment Type - Container: Using Docker)

1.Start an instruction using any of the playwright command.
2. Return this error:
COMMANDS:
{'scrape_text': {'url': 'REDACTED'}}

Error: It looks like you are using Playwright Sync API inside the asyncio loop.
Please use the Async API instead.",['commands/web_playwright.py'],['fail-stops']
137,CloudDevStudios/Agent-LLM,https://github.com/CloudDevStudios/Agent-LLM/tree/f89aaebd761a4d016b8a61ee511330f5ed403fe9,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,CustomPrompt.py,"1.In the UI, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""",['CustomPrompt.py'],['incorrectness']
138,CloudDevStudios/Agent-LLM,https://github.com/CloudDevStudios/Agent-LLM/tree/f89aaebd761a4d016b8a61ee511330f5ed403fe9,missing llm input format validation,/,TypeError: Object of type bytes is not JSON serializable(pdf),IC,app.py,"click ""Interact"", choose ""Learning"", choose agent, method from file, choose a pdf and upload",['app.py'],['incorrectness']
139,CloudDevStudios/Agent-LLM,https://github.com/CloudDevStudios/Agent-LLM/tree/f89aaebd761a4d016b8a61ee511330f5ed403fe9,insufficient history management,/,Stuck creating memories ,CL,AgentLLM.py,"1.streamlit run Main.py

2.Run textgen with: python server.py --listen-port 7861 --wbits 4 --groupsize 128 --model llama-13b-4bit-128g --model_type llama --xformers --api
See that it is running:
Starting streaming server at ws://127.0.0.1:5005/api/v1/stream Starting API at http://127.0.0.1:5000/api Running on local URL:  http://127.0.0.1:7861

3.Create new agent with config:

{""commands"": {""Read Audio from File"": false, ""Read Audio"": false, ""Evaluate Code"": true, ""Analyze Pull Request"": false, ""Perform Automated Testing"": false, ""Run CI-CD Pipeline"": false, ""Improve Code"": false, ""Write Tests"": false, ""Create a new command"": false, ""Execute Python File"": false, ""Execute Shell"": false, ""Read File"": true, ""Write to File"": true, ""Append to File"": true, ""Delete File"": true, ""Search Files"": true, ""Clone Github Repository"": false, ""Google Search"": true, ""Searx Search"": false, ""Get Datetime"": false, ""Speak with TTS"": false, ""Scrape Text with Playwright"": false, ""Scrape Links with Playwright"": false, ""Is Valid URL"": false, ""Sanitize URL"": false, ""Check Local File Access"": false, ""Get Response"": true, ""Scrape Text"": true, ""Scrape Links"": true, ""Scrape Text with Selenium"": false, ""Ask AI Agent gpt4free"": false, ""Instruct AI Agent gpt4free"": false, ""Ask AI Agent Ooga"": false, ""Instruct AI Agent Ooga"": false}, ""settings"": {""provider"": ""oobabooga"", ""AI_MODEL"": ""gpt-3.5-turbo"", ""AI_TEMPERATURE"": ""0.4"", ""MAX_TOKENS"": ""4000"", ""embedder"": ""default"", ""AI_PROVIDER_URI"": ""http://127.0.0.1:5000"", ""HUGGINGFACE_AUDIO_TO_TEXT_MODEL"": ""facebook/wav2vec2-large-960h-lv60-self"", ""DISCORD_COMMAND_PREFIX"": ""/AGiXT"", ""WORKING_DIRECTORY"": ""./WORKSPACE"", ""WORKING_DIRECTORY_RESTRICTED"": ""True"", ""SEARXNG_INSTANCE_URL"": ""https://searx.work"", ""USE_BRIAN_TTS"": ""True"", ""ELEVENLABS_VOICE"": ""Josh"", ""SELENIUM_WEB_BROWSER"": ""chrome"", """": """"}}

4.Ask it to do anything or chat with it.
5.Behavior: Agent is unable to do anything, program is stuck trying to create memories.",['AgentLLM.py'],['slower execution']
140,CloudDevStudios/Agent-LLM,https://github.com/CloudDevStudios/Agent-LLM/tree/f89aaebd761a4d016b8a61ee511330f5ed403fe9,imprecise knowledge retrieval,/,Unable to retrieve data ,"CL,SL",commands/google.py,"1.Setup with docker.  (Local)
2.Run Gen and run Task Chain.",['commands/google.py'],['slower execution']
141,CloudDevStudios/Agent-LLM,https://github.com/CloudDevStudios/Agent-LLM/tree/f89aaebd761a4d016b8a61ee511330f5ed403fe9,resource contention,/,Playwright Sync API inside the asyncio loop error,ST,commands/web_playwright.py,"(Environment Type - Connection: Local;  Environment Type - Container: Using Docker)

1.Start an instruction using any of the playwright command.
2. Return this error:
COMMANDS:
{'scrape_text': {'url': 'REDACTED'}}

Error: It looks like you are using Playwright Sync API inside the asyncio loop.
Please use the Async API instead.",['commands/web_playwright.py'],['fail-stops']
142,ChuloAI/BrainChulo,https://github.com/ChuloAI/BrainChulo/tree/f2d6753177ef9940dcb6994da03fbf5fd323a3b3,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,"app/main.py
app/conversations/document_based.py/ __init__(self)
app/prompt_templates/document_based_conversation.py","

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk","['app/main.py', 'app/prompt_templates/document_based_conversation.py', 'app/conversations/document_based.py']",['fail-stops']
143,daveshap/ChromaDB_Chatbot_Public,https://github.com/daveshap/ChromaDB_Chatbot_Public/tree/ad7df3a7391c07ab59298e48ac05c24c8698698b,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,chat.py,"1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""",['chat.py'],['incorrectness']
144,daveshap/ChromaDB_Chatbot_Public,https://github.com/daveshap/ChromaDB_Chatbot_Public/tree/ad7df3a7391c07ab59298e48ac05c24c8698698b,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,chat.py,Upload the pptx file.,['chat.py'],['incorrectness']
145,daveshap/ChromaDB_Chatbot_Public,https://github.com/daveshap/ChromaDB_Chatbot_Public/tree/ad7df3a7391c07ab59298e48ac05c24c8698698b,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,chat.py,"

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk",['chat.py'],['fail-stops']
146,AgentValley/chromadb-chatbot,https://github.com/AgentValley/chromadb-chatbot/tree/48c10aa15ae4c0896fc282c4f1c4efdfccbd53bf,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"routes/message.py
tools/chat_openai.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""","['routes/message.py', 'tools/chat_openai.py']",['incorrectness']
147,AgentValley/chromadb-chatbot,https://github.com/AgentValley/chromadb-chatbot/tree/48c10aa15ae4c0896fc282c4f1c4efdfccbd53bf,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,"routes/message.py
tools/chat_openai.py",Upload the pptx file.,"['routes/message.py', 'tools/chat_openai.py']",['incorrectness']
148,AgentValley/chromadb-chatbot,https://github.com/AgentValley/chromadb-chatbot/tree/48c10aa15ae4c0896fc282c4f1c4efdfccbd53bf,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,"chatbot/runner.py
const.py
tools/load_data.py
tools/chat_openai.py","

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk","['const.py', 'tools/load_data.py', 'tools/chat_openai.py', 'chatbot/runner.py']",['fail-stops']
149,edrickdch/chat-pdf,https://github.com/edrickdch/chat-pdf/tree/bf5343fef116a360424245b1c154edc2bac07bfd,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,src/single-pdf.py,"1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""",['src/single-pdf.py'],['incorrectness']
150,edrickdch/chat-pdf,https://github.com/edrickdch/chat-pdf/tree/bf5343fef116a360424245b1c154edc2bac07bfd,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,src/single-pdf.py,Upload the pptx file.,['src/single-pdf.py'],['incorrectness']
151,edrickdch/chat-pdf,https://github.com/edrickdch/chat-pdf/tree/bf5343fef116a360424245b1c154edc2bac07bfd,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,src/single-pdf.py,"

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk",['src/single-pdf.py'],['fail-stops']
152,Gamma-Software/AppifyAi,https://github.com/Gamma-Software/AppifyAi/tree/6c6a116d2530cbf4cb83809a931ca0a1f86c23f2,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"generative_app/core/chains/prompt.py
generative_app/core/chains/conversational_retrieval_over_code.py
generative_app/core/chains/llm.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""","['generative_app/core/chains/conversational_retrieval_over_code.py', 'generative_app/core/chains/llm.py', 'generative_app/core/chains/prompt.py']",['incorrectness']
153,yujiosaka/ChatIQ,https://github.com/yujiosaka/ChatIQ/tree/def21481b0c132173b51d1d7a314b103db324ca2,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"chatiq/prompt.py
chatiq/chat_chain.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""","['chatiq/chat_chain.py', 'chatiq/prompt.py']",['incorrectness']
154,yujiosaka/ChatIQ,https://github.com/yujiosaka/ChatIQ/tree/def21481b0c132173b51d1d7a314b103db324ca2,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,"chatiq/prompt.py
chatiq/chat_chain.py",Upload the pptx file.,"['chatiq/chat_chain.py', 'chatiq/prompt.py']",['incorrectness']
155,yujiosaka/ChatIQ,https://github.com/yujiosaka/ChatIQ/tree/def21481b0c132173b51d1d7a314b103db324ca2,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,"chatiq/text_processor.py
chatiq/chatiq.py","

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk","['chatiq/chatiq.py', 'chatiq/text_processor.py']",['fail-stops']
156,labring/FastGPT,https://github.com/labring/FastGPT/tree/05bf1b22653f8699b85098db3e86a7f29bdc2895,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,"projects/app/src/pages/api/core/ai/token.ts
files/models/Baichuan2/openai_api.py
python/ocr/surya/app.py","

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk",['files/models/Baichuan2/openai_api.py'],['fail-stops']
157,c121914yu/FastGPT,https://github.com/c121914yu/FastGPT/tree/2ae8d43216d4e6fb739143c3cc12285fed048596,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,"projects/app/src/pages/api/core/ai/token.ts
files/models/ChatGLM2/openai_api.py
python/bge-rerank/bge-reranker-base/app.py","

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk",['files/models/ChatGLM2/openai_api.py'],['fail-stops']
158,Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,"chat.py
Notebook/GPT-3_customvectorsearch.ipynb","

1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk",['chat.py'],['fail-stops']
159,Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,missing llm input format validation,case1,The analysis of PPT documents needs optimization,IC,"chat.py
app.py","1. Use app online
2. Create a new knowledge base.
3. In the ""Dataset"" interface, select the ""Create/Import"" tab and choose ""Text Dataset.""
4. Upload the pptx file.
5. After parsing the pptx file, the order is chaotic.","['app.py', 'chat.py']",['incorrectness']
160,Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,knowledge misalignment,case1,"During file segmentation, if the number of characters under a level 3 heading (###) is less than 29, the level 3 heading will be lost, and its content will be directly included in the next segment.",IC,chat.py,"1.Upload a .docx file (containing level 3 headings) to your knowledge base.
2.In ""Data Processing,"" select ""Direct Segmentation"" and ""Automatic.""
3.In the segmentation results, if a level 3 heading (###) has fewer than 29 characters, the heading is lost, and its content is directly merged into the next segment. If it has 29 or more characters, the issue doesn't occur.",['chat.py'],['incorrectness']
161,Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,imprecise knowledge retrieval,case1,"In ""knowledge base search test"", for Q&A based on Excel spreadsheets, if the Excel file is large, the answers may be inaccurate.",IC,"chat.py
app.py","1.Upload an xlsx file with around 10,000 rows to your knowledge base.
2.In ""Search Test,"" ask questions related to the content of this file.
3.In ""Knowledge Base Search Configuration,"" select different search modes to test the Q&A, but the test results are unsatisfactory, with inaccurate answers.","['app.py', 'chat.py']",['incorrectness']
162,Abonia1/Context-Based-LLMChatbot,https://github.com/Abonia1/Context-Based-LLMChatbot/tree/f21f7e157157d235e0b9ddd2fc997a7d33096da1,unclear context in prompt,/,"In ""workflow"", the knowledge base reference accessed by the HTTP module does not display the referenced content in the conversation.",IC,"config.py
chat.py","1.(A conversational application) IN ""workflow"" , Integrate knowledge base citations into the HTTP module.
2.During debugging, when the bot answers questions related to the knowledge base, it does not cite references.","['config.py', 'chat.py']",['incorrectness']
163,h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,"src/gpt4all_llm.py
openai_server/server.py","

1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk",['src/gpt4all_llm.py'],['fail-stops']
164,h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,inefficient memory management,case1,Processing of large PDFS (100k) is slow,SL,"models/gpu_mem_track.py
src/utils.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Configure the system to handle large PDFs.
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Upload a large PDF document (around 100k pages or similar in size) for processing.
5. Monitor the processing time.
6. Observe if the processing is slow and note the time taken to process the large PDF.(for example, ValueError: invalid literal for int() with base 10: 'some_value')","['src/utils.py', 'models/gpu_mem_track.py']",['slower execution']
165,h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,imprecise knowledge retrieval,/,Mismatch Between Query Response and Source Document Ordering in a Captive Network with Multiple ,"ST,TK","openai_server/server.py
src/utils.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Configure the system to connect to a captive network with multiple source documents.
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Upload multiple source documents to the captive network.
5. Open the query interface in the web UI.
6. Enter a query related to the content of the source documents.
7. Observe if there is a mismatch between the query response and the ordering of the information in the source documents.(for example, ""TypeError: 'NoneType' object is not iterable"")",['src/utils.py'],"['fail-stops', 'more tokens']"
166,h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,privacy violation,/,an illegal memory access was encountered ,IS,src/create_data.py,"1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Prepare the system with a dataset or configuration that may trigger memory access issues.
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that processes a large dataset or requires significant computational resources.
6. Observe if an error message indicating ""an illegal memory access was encountered"" occurs during the processing.",['src/create_data.py'],['insecure']
167,h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,insufficient history management,/,"without filtering data stuffed into context, it pushes LLM into bad outputs ",IC,"src/prompter.py
openai_server/server.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Upload or configure a dataset that includes noisy or irrelevant data.
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that retrieves information from the dataset.
6. Observe if the LLM produces bad outputs due to the noisy or irrelevant data being included in the context without filtering.",['src/prompter.py'],['incorrectness']
168,h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,lacking restrictions in prompt,case1,Limited Max output tokens,IC,"src/gpt4all_llm.py
openai_server/server.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Configure the maximum output tokens limit in the configuration file.
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that requires a detailed and lengthy response.
6. Observe if the response is cut off or limited in length due to the maximum output tokens limit configured.(for example, ModuleNotFoundError: No module named 'some_module')",['src/gpt4all_llm.py'],['incorrectness']
169,h2oai/h2ogpt,https://github.com/h2oai/h2ogpt/tree/15f3a2d564541020f78cfc83bc90722dcda8a960,unclear context in prompt,case1,No DB is used despite it's existing,IC,"models/gpu_mem_track.py
src/utils.py","1. Set up h2ogpt: Ensure the project is correctly set up in your local environment.
2. Configure the system to connect to an existing database.
3. Run h2ogpt: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that should retrieve data from the database.
6. Observe if the response utilizes the database for retrieving information or if the database is not used despite its existence. Monitor server logs or terminal output for any indications that the database is not being accessed.","['src/utils.py', 'models/gpu_mem_track.py']",['incorrectness']
170,Jerk400/h2ogpt-dev-local,https://github.com/Jerk400/h2ogpt-dev-local/tree/ba56d6471080ec447c7c5347a2d4765ae4dba5b4,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"src/prompter.py
src/h2oai_pipeline.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""","['src/prompter.py', 'src/h2oai_pipeline.py']",['incorrectness']
171,Jerk400/h2ogpt-dev-local,https://github.com/Jerk400/h2ogpt-dev-local/tree/ba56d6471080ec447c7c5347a2d4765ae4dba5b4,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,"models/modelling_RW_falcon7b.py
src/gen.py",Upload the pptx file.,"['models/modelling_RW_falcon7b.py', 'src/gen.py']",['incorrectness']
172,Jerk400/h2ogpt-dev-local,https://github.com/Jerk400/h2ogpt-dev-local/tree/ba56d6471080ec447c7c5347a2d4765ae4dba5b4,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,src/enums.py,"1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk",['src/enums.py'],['fail-stops']
173,ushakrishnan/SearchWithOpenAI,https://github.com/ushakrishnan/SearchWithOpenAI/tree/470a5d10f786efbac2feab18330c69365a937599,exceeding llm content limit,exceeding  LLM content limit,1. Glitch when indexing 32mb PDF file.,ST,"common/funs.py/addtostorepdf()
pages/3_Index_PDFnTXT_Into_ChromaDB_Store.py/start_capture()","1.Set up SearchWithOpenAI according to the README.md of this project: https://github.com/ushakrishnan/SearchWithOpenAI/README.md
2. Run: ""streamlit run Home.py""
(
(1)Prompt:
how wood cribs are fastened?
st.warningmethodstreamlit.delta_generator.AlertMixin.warning(body: 'SupportsStr', *, icon: Optional[str] = None) -> 'DeltaGenerator'
Display warning message.

(2)Parameters:
body : str
The warning text to display.
icon : str or None
An optional, keyword-only argument that specifies an emoji to use as
the icon for the alert. Shortcodes are not allowed, please use a
single character instead. E.g. ""🚨"", ""🔥"", ""🤖"", etc.
Defaults to None, which means no icon is displayed.

(3)Documents Persisted in Vector Database: please see: https://github.com/ushakrishnan/SearchWithOpenAI/issues/3)

3. Runs into this error:  IndexError: list index out of range","['common/funs.py', 'pages/3_Index_PDFnTXT_Into_ChromaDB_Store.py']",['fail-stops']
174,EmbraceAGI/LocalAGI,https://github.com/EmbraceAGI/LocalAGI/tree/f2fd7824ee270b8d45b336366ec885a14b809b8b,absence of final output,absence of final output,The program cannot stop.,UI,"local_agi.py
local_agi_zh.py","1. Set up LocalAGI according to the README.md of this project: https://github.com/EmbraceAGI/LocalAGI/blob/main/README.md
2. Set `.env` with:
   LLM_MODEL=chatglm-6b
   OBJECTIVE=Implement a Python code to accept input and print it in the terminal
   INITIAL_TASK=Build a task list
3. And then LocalAGI starts creating and completing tasks in an infinite loop, never stopping.","['local_agi_zh.py', 'local_agi.py']",['unfriendly user interface']
175,EmbraceAGI/LocalAGI,https://github.com/EmbraceAGI/LocalAGI/tree/f2fd7824ee270b8d45b336366ec885a14b809b8b,imprecise knowledge retrieval,/,"There is an issue with how the vector database is queried. The execution_agent function currently uses the objective parameter to query similar documents. The query needs use the task parameter instead, as this would more effectively leverage existing solutions relevant to the specific task being addressed.","IC,SL","local_agi.py
local_agi_mini.py","1.Clone the repository via git clone and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.(Optional) Set the objective of the task management system in the OBJECTIVE variable.
8.(Optional) Set the first task of the system in the INITIAL_TASK variable.
9.Run the script","['local_agi_mini.py', 'local_agi.py']","['incorrectness', 'slower execution']"
176,EmbraceAGI/LocalAGI,https://github.com/EmbraceAGI/LocalAGI/tree/f2fd7824ee270b8d45b336366ec885a14b809b8b,unnecessary llm output,/,The task creation process generating too many unnecessary tasks,"IC,UI","local_agi.py
local_agi_zh.py","1.Clone the Repository
2.Install the Required Packages:
   pip install -r requirements.txt
3.Set Up Environment Variables:
  (1)Copy the .env.example file to .env:
   cp .env.example .env
  (2)Open the .env file and set the following variables:
   OPENAI_API_KEY=your_openai_api_key
   OPENAI_API_MODEL=gpt-3.5-turbo
   TABLE_NAME=your_results_store_name
   COOPERATIVE_MODE=local
(3)Optionally, set other variables:
   BABY_NAME=your_babyagi_instance_name
   OBJECTIVE=""do nothing""
   INITIAL_TASK=""Make a todo list""
4.Run the Script
5.Observe Tasks and Outcomes","['local_agi_zh.py', 'local_agi.py']","['incorrectness', 'unfriendly user interface']"
177,EmbraceAGI/LocalAGI,https://github.com/EmbraceAGI/LocalAGI/tree/f2fd7824ee270b8d45b336366ec885a14b809b8b,incompatible llm output format,/,Task list numbering continues to reset or get misordered.,IC,"local_agi.py
local_agi_zh.py","1.Run the script according to the previously mentioned steps.
2.Observe whether the generated task list shows any numbering inconsistencies or misorder.","['local_agi_zh.py', 'local_agi.py']",['incorrectness']
178,EmbraceAGI/LocalAGI,https://github.com/EmbraceAGI/LocalAGI/tree/f2fd7824ee270b8d45b336366ec885a14b809b8b,insufficient history management,/,Task creation agent ignores task lists in previous task results.,IC,"local_agi.py
local_agi_mini.py
local_agi_zh.py","1.Clone the repository via git clone https://github.com/yoheinakajima/babyagi.git and cd into the cloned repository.
2.Install the required packages: pip install -r requirements.txt
3.Copy the .env.example file to .env: cp .env.example .env. This is where you will set the following variables.
4.Set your OpenAI API key in the OPENAI_API_KEY and OPENAI_API_MODEL variables. In order to use with Weaviate you will also need to setup additional variables detailed here.
5.Set the name of the table where the task results will be stored in the TABLE_NAME variable.
6.(Optional) Set the name of the BabyAGI instance in the BABY_NAME variable.
7.Set the OBJECTIVE variable as ""Have a fun day.""
8.Set the INITIAL_TASK variable as ""1.Make a list of tasks"".
9.Run the script: python babyagi.py
10.After several rounds of task generation, check whether the newly generated task lists ignore the preceding task results.","['local_agi_zh.py', 'local_agi_mini.py', 'local_agi.py']",['incorrectness']
179,StanGirard/quivr,https://github.com/StanGirard/quivr/tree/b6f38f7aff5f026eb46a071362c9362e057119df,imprecise knowledge retrieval,case1,Knowledge fetching is very slow,SL,"backend/models/databases/supabase/brains.py/delete_file_from_brain()
backend/routes/knowledge_routes.py","1. Use Quivr online or deploy locally following instructions on https://github.com/QuivrHQ/quivr/README.md
2. Create a brain, upload multiple files to its knowledge. Ask Quivr questions about the documents you uploaded.
3. Check if it takes too long to index files.","['backend/models/databases/supabase/brains.py', 'backend/routes/knowledge_routes.py']",['slower execution']
180,StanGirard/quivr,https://github.com/StanGirard/quivr/tree/b6f38f7aff5f026eb46a071362c9362e057119df,insufficient history management,/,Quiver does not shows all the records that I aske to show in CSV file,IC,backend/tests/test_upload.py,"1. Use Quivr online or deploy locally following instructions on https://github.com/QuivrHQ/quivr/README.md
2. Upload a CSV-formatted bank statement onto Quivr Brain (contains at least 20 instances) and requested Quivr to display some specific instances where you made a payment of XXX.
3. Despite there being over 20 eligible instances , Quivr only presents 4 records.",['backend/tests/test_upload.py'],['incorrectness']
181,arc53/DocsGPT,https://github.com/arc53/DocsGPT/tree/744d4ebbaf4349444d8d361593c4c04652c6c54b,incompatible llm output format,/,Bug Report: DocsGpt result bullets are not aligned properly,IC,scripts/parser/file/html_parser.py,"1.Set up DocsGPT: Ensure the DocsGPT project is correctly set up in your local environment.
2.Ask a Question: Use DocsGPT to ask any question that generates a response with bullet points.
3.Observe the Bullets: Notice that the bullets in the response are not aligned properly.",['scripts/parser/file/html_parser.py'],['incorrectness']
182,arc53/DocsGPT,https://github.com/arc53/DocsGPT/tree/744d4ebbaf4349444d8d361593c4c04652c6c54b,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,src/enums.py,"1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk",['src/enums.py'],['fail-stops']
183,yangchuansheng/DocsGPT,https://github.com/yangchuansheng/DocsGPT/tree/595581b624b54046d756ad28bc845521ee4807d0,incompatible llm output format,/,Bug Report: DocsGpt result bullets are not aligned properly,IC,scripts/parser/file/rst_parser.py,"1.Set up DocsGPT: Ensure the DocsGPT project is correctly set up in your local environment.
2.Ask a Question: Use DocsGPT to ask any question that generates a response with bullet points.
3.Observe the Bullets: Notice that the bullets in the response are not aligned properly.",['scripts/parser/file/rst_parser.py'],['incorrectness']
184,yangchuansheng/DocsGPT,https://github.com/yangchuansheng/DocsGPT/tree/595581b624b54046d756ad28bc845521ee4807d0,unclear context in prompt,/,text overflow in chat's title,IC,"application/app.py
scripts/ingest.py","1.Set up DocsGPT: Ensure the DocsGPT project is correctly set up in your local environment.
2.Initiate a Chat: Start a chat session within DocsGPT.
3.Create a Long Title: Generate a chat session title that is long enough to cause overflow.
4.Observe Text Overflow: Notice that the chat's title text overflows and is not contained within the title box.","['application/app.py', 'scripts/ingest.py']",['incorrectness']
185,yangchuansheng/DocsGPT,https://github.com/yangchuansheng/DocsGPT/tree/595581b624b54046d756ad28bc845521ee4807d0,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,scripts/old/ingest_rst.py,"1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk",['scripts/old/ingest_rst.py'],['fail-stops']
186,DanielSmith0831/DocsGPT,https://github.com/NinjaDevOps0831/DocsGPT/tree/6b6737613ad934a28ae4251ffa9716465e6e35dd,incompatible llm output format,/,Bug Report: DocsGpt result bullets are not aligned properly,IC,scripts/parser/file/html_parser.py,"1.Set up DocsGPT: Ensure the DocsGPT project is correctly set up in your local environment.
2.Ask a Question: Use DocsGPT to ask any question that generates a response with bullet points.
3.Observe the Bullets: Notice that the bullets in the response are not aligned properly.",['scripts/parser/file/html_parser.py'],['incorrectness']
187,DanielSmith0831/DocsGPT,https://github.com/NinjaDevOps0831/DocsGPT/tree/6b6737613ad934a28ae4251ffa9716465e6e35dd,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,src/enums.py,"

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk",['src/enums.py'],['fail-stops']
188,chatchat-space/Langchain-Chatchat,https://github.com/chatchat-space/Langchain-Chatchat/tree/bc7f01925fe49c622cf7d6e2540c03fd5a0679e1,improper text embedding,/,Some markdown documents fail to vectorize when used as a database,"IC,ST","libs/chatchat-server/chatchat/server/localai_embeddings.py/embed_with_retry
libs/python-sdk/open_chatcaht/api/knowledge_base/knowledge_base_client.py/recreate_vector_store
libs/chatchat-server/chatchat/webui_pages/utils.py/recreate_vector_store
libs/chatchat-server/chatchat/server/knowledge_base/kb_cache/faiss_cache.py/faiss_cache.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment with the necessary dependencies.
2.Use Elasticsearch for Vector Database: Configure Elasticsearch as the vector database.
3.Add Markdown Documents: Add multiple markdown documents to the knowledge base.
4.Observe Failures: Notice that some markdown documents fail to vectorize properly, with errors like ""index creation failed"" and ""index already exists.""","['chatchat-server/chatchat_server/knowledge_base/kb_cache/faiss_cache.py', 'webui-pages/chatchat_webui_pages/dialogue/utils.py', 'chatchat-server/chatchat_server/localai_embeddings.py', 'webui-pages/chatchat_webui_pages/utils.py']","['incorrectness', 'fail-stops']"
189,hzg0601/langchain-ChatGLM-annotation,https://github.com/hzg0601/langchain-ChatGLM-annotation/tree/da8085ba5fcb89a2e665cd3fde28cdafa38b9c29,missing llm input format validation,/,"In linux, an error occurs when the.doc file is uploaded and parsed. Therefore, the.doc file cannot be parsed",ST IC,"agent/custom_agent.py
models/llama_llm.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Use Linux Environment: Make sure you are operating within a Linux environment.
3.Upload a .doc File: Try uploading a .doc file to the knowledge base.
4.Observe the Error: Notice the error message indicating that the .doc file format is not supported and the file cannot be parsed.","['agent/custom_agent.py', 'models/llama_llm.py']","['incorrectness', 'fail-stops']"
190,hzg0601/langchain-ChatGLM-annotation,https://github.com/hzg0601/langchain-ChatGLM-annotation/tree/da8085ba5fcb89a2e665cd3fde28cdafa38b9c29,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,"models/llama_llm.py
models/chatglm_llm.py","1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk","['models/chatglm_llm.py', 'models/llama_llm.py']",['fail-stops']
191,ExpressGit/langchain-ChatGLM,https://github.com/ExpressGit/langchain-ChatGLM/tree/21035d7706b24956f845c65e1492690abbb07d8b,improper text embedding,/,Some markdown documents fail to vectorize when used as a database,"IC,ST","chains/modules/embeddings.py
chains/modules/vectorstores.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment with the necessary dependencies.
2.Use Elasticsearch for Vector Database: Configure Elasticsearch as the vector database.
3.Add Markdown Documents: Add multiple markdown documents to the knowledge base.
4.Observe Failures: Notice that some markdown documents fail to vectorize properly, with errors like ""index creation failed"" and ""index already exists.""","['chains/modules/embeddings.py', 'chains/modules/vectorstores.py']","['incorrectness', 'fail-stops']"
192,ExpressGit/langchain-ChatGLM,https://github.com/ExpressGit/langchain-ChatGLM/tree/21035d7706b24956f845c65e1492690abbb07d8b,missing llm input format validation,/,"In linux, an error occurs when the.doc file is uploaded and parsed. Therefore, the.doc file cannot be parsed",ST IC,"models/base.py
models/llama_llm.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Use Linux Environment: Make sure you are operating within a Linux environment.
3.Upload a .doc File: Try uploading a .doc file to the knowledge base.
4.Observe the Error: Notice the error message indicating that the .doc file format is not supported and the file cannot be parsed.","['models/base.py', 'models/llama_llm.py']","['incorrectness', 'fail-stops']"
193,ExpressGit/langchain-ChatGLM,https://github.com/ExpressGit/langchain-ChatGLM/tree/21035d7706b24956f845c65e1492690abbb07d8b,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,"models/llama_llm.py
models/base.py","

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk","['models/base.py', 'models/llama_llm.py']",['fail-stops']
194,zhjunqin/Langchain-Chatchat-InternLM,https://github.com/zhjunqin/Langchain-Chatchat-InternLM/tree/761f0bc3a94ccdda9970761b6c769c7e0537e2e8,improper text embedding,/,Some markdown documents fail to vectorize when used as a database,"IC,ST","server/knowledge_base/kb_service/base.py
server/knowledge_base/utils.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment with the necessary dependencies.
2.Use Elasticsearch for Vector Database: Configure Elasticsearch as the vector database.
3.Add Markdown Documents: Add multiple markdown documents to the knowledge base.
4.Observe Failures: Notice that some markdown documents fail to vectorize properly, with errors like ""index creation failed"" and ""index already exists.""","['server/knowledge_base/utils.py', 'server/knowledge_base/kb_service/base.py']","['incorrectness', 'fail-stops']"
195,zhjunqin/Langchain-Chatchat-InternLM,https://github.com/zhjunqin/Langchain-Chatchat-InternLM/tree/761f0bc3a94ccdda9970761b6c769c7e0537e2e8,unclear context in prompt,/,Knowledge Base Q&A does not match a concise description of/problem,IC,"chains/llmchain_with_history.py
server/chat/chat.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Execute 'Knowledge Base Management - Create New Knowledge Base - Upload Files'.
3.Click 'Conversation - Select Knowledge Base Q&A - Choose Knowledge Base'.
4.Scroll to 'Input Box and Ask a Question'.
5.Observe the issue: 'No related documents found, the response is based on the model's own capability.'","['chains/llmchain_with_history.py', 'server/chat/chat.py']",['incorrectness']
196,zhjunqin/Langchain-Chatchat-InternLM,https://github.com/zhjunqin/Langchain-Chatchat-InternLM/tree/761f0bc3a94ccdda9970761b6c769c7e0537e2e8,missing llm input format validation,/,"In linux, an error occurs when the.doc file is uploaded and parsed. Therefore, the.doc file cannot be parsed",ST IC,"server/chat/chat.py
chains/llmchain_with_history.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Use Linux Environment: Make sure you are operating within a Linux environment.
3.Upload a .doc File: Try uploading a .doc file to the knowledge base.
4.Observe the Error: Notice the error message indicating that the .doc file format is not supported and the file cannot be parsed.","['chains/llmchain_with_history.py', 'server/chat/chat.py']","['incorrectness', 'fail-stops']"
197,zhjunqin/Langchain-Chatchat-InternLM,https://github.com/zhjunqin/Langchain-Chatchat-InternLM/tree/761f0bc3a94ccdda9970761b6c769c7e0537e2e8,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,"server/chat/search_engine_chat.py
server/chat/knowledge_base_chat.py","1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk","['server/chat/knowledge_base_chat.py', 'server/chat/search_engine_chat.py']",['fail-stops']
198,menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,exceeding llm content limit,case1,1.too many tokens ,ST,"app.py
privateGPT.py","1.Set up PrivateGPT: Ensure the PrivateGPT project is correctly set up in your local environment.
2.Configure Environment Variables: Set MODEL_N_CTX and MODEL_N_BATCH in the .env file.
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Query: Input a simple query such as ""hi"".
5.Observe the Error: Notice the error message ""too many tokens"" indicating the context window is exceeded.","['privateGPT.py', 'app.py']",['fail-stops']
199,menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,incompatible llm output format,case1,Answers contain additional prompts with certain models ,IC,"app.py
privateGPT.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Configure Environment Variables: Set the environment variables, including MODEL_PATH to a specific model path (e.g., models/vicuna_ggml-vicuna-7b-1.1/ggml-vic7b-uncensored-q4_0.bin).
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Query: Input a query such as ""What is a tree?"".
5.Observe the Response: Notice that the response contains additional prompts or unrelated names and characters.","['privateGPT.py', 'app.py']",['incorrectness']
200,menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,unclear context in prompt,case1,Sources are not being used,IC,"app.py
privateGPT.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Use the GUI: Open the web UI and upload a document.
4. Ask a Question: Enter a question that should be answerable from the document.
5. Observe the Response: Note that the model does not utilize the supplied documents' information but instead relies on its own knowledge.","['privateGPT.py', 'app.py']",['incorrectness']
201,menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,lacking restrictions in prompt,case1,If a Document fails to process; ingesting should continue,"ST,UI","app.py
privateGPT.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Configure the Environment: Set the `MAX_HISTORY` parameter to a specific value, such as 10.
3. Run PrivateGPT: Execute `python privateGPT.py`.
4. Open the chat interface in the web UI.
5. Enter a series of queries exceeding the `MAX_HISTORY` value.
6. Observe the chat history: Notice if older messages are not being removed as expected or if there is an issue with the history management.","['privateGPT.py', 'app.py']","['fail-stops', 'unfriendly user interface']"
202,menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,imprecise knowledge retrieval,/,ingest does not complete,"IC,SL","streamlit_app.py
app.py
privateGPT.py","1. Set up PrivateGPT: Ensure the project is correctly configured in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Open the GUI: Access the web UI.
4. Attempt to Upload a File: Try uploading a file with a size greater than 50MB.
5. Observe the Behavior: Notice if an error occurs or if the upload fails.","['privateGPT.py', 'streamlit_app.py', 'app.py']","['incorrectness', 'slower execution']"
203,menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,inefficient memory management,case1,Not Enough Space on every Inquiry · Issue ,ST,"streamlit_app.py
app.py
privateGPT.py","1. Set up PrivateGPT: Ensure the project is correctly configured in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Open the GUI: Access the web UI.
4. Initiate a New Chat: Start a new chat session.
5. Enter a Query: Type a query and submit it.
6. Observe the Response: Notice if the AI's response is missing or incomplete.","['privateGPT.py', 'streamlit_app.py', 'app.py']",['fail-stops']
204,menloparklab/privateGPT-app,https://github.com/menloparklab/privateGPT-app/tree/028c81038dec2f923477812f7c7f7d48706fa7a9,resource contention,case1,Set n_threads drastically slow down privateGPT ,SL,"streamlit_app.py
app.py
privateGPT.py","Ensure PrivateGPT is set up and the server is running.
Open the privateGPT.py file.
Set the n_threads parameter to a high value, such as n_threads=40, to use all CPU cores.
Save the changes.
Restart the PrivateGPT server.
Observe the server's performance and note if there is a drastic slowdown or significant delay in response times when answering questions.
Additional Steps:

Try setting n_threads=1 to use only 1 CPU core and observe if the performance issue persists.
Remove the n_threads parameter entirely from the privateGPT.py file and check if the answer speed improves and returns to normal.","['privateGPT.py', 'streamlit_app.py', 'app.py']",['slower execution']
205,AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,missing llm input format validation,/,"In linux, an error occurs when the.doc file is uploaded and parsed. Therefore, the.doc file cannot be parsed",ST IC,"privateGPT.py
ingest.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Use Linux Environment: Make sure you are operating within a Linux environment.
3.Upload a .doc File: Try uploading a .doc file to the knowledge base.
4.Observe the Error: Notice the error message indicating that the .doc file format is not supported and the file cannot be parsed.","['privateGPT.py', 'ingest.py']","['incorrectness', 'fail-stops']"
206,AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,exceeding llm content limit,case1,1.too many tokens ,ST,"privateGPT.py
ingest.py","1.Set up PrivateGPT: Ensure the PrivateGPT project is correctly set up in your local environment.
2.Configure Environment Variables: Set MODEL_N_CTX and MODEL_N_BATCH in the .env file.
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Query: Input a simple query such as ""hi"".
5.Observe the Error: Notice the error message ""too many tokens"" indicating the context window is exceeded.","['privateGPT.py', 'ingest.py']",['fail-stops']
207,AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,incompatible llm output format,case1,Answers contain additional prompts with certain models ,IC,"privateGPT.py
ingest.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Configure Environment Variables: Set the environment variables, including MODEL_PATH to a specific model path (e.g., models/vicuna_ggml-vicuna-7b-1.1/ggml-vic7b-uncensored-q4_0.bin).
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Query: Input a query such as ""What is a tree?"".
5.Observe the Response: Notice that the response contains additional prompts or unrelated names and characters.","['privateGPT.py', 'ingest.py']",['incorrectness']
208,AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,unclear context in prompt,case1,Sources are not being used,IC,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Use the GUI: Open the web UI and upload a document.
4. Ask a Question: Enter a question that should be answerable from the document.
5. Observe the Response: Note that the model does not utilize the supplied documents' information but instead relies on its own knowledge.","['privateGPT.py', 'ingest.py']",['incorrectness']
209,AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,lacking restrictions in prompt,case1,If a Document fails to process; ingesting should continue,"ST,UI","privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Configure the Environment: Set the `MAX_HISTORY` parameter to a specific value, such as 10.
3. Run PrivateGPT: Execute `python privateGPT.py`.
4. Open the chat interface in the web UI.
5. Enter a series of queries exceeding the `MAX_HISTORY` value.
6. Observe the chat history: Notice if older messages are not being removed as expected or if there is an issue with the history management.","['privateGPT.py', 'ingest.py']","['fail-stops', 'unfriendly user interface']"
210,AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,imprecise knowledge retrieval,/,ingest does not complete,"IC,SL","privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly configured in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Open the GUI: Access the web UI.
4. Attempt to Upload a File: Try uploading a file with a size greater than 50MB.
5. Observe the Behavior: Notice if an error occurs or if the upload fails.","['privateGPT.py', 'ingest.py']","['incorrectness', 'slower execution']"
211,AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,inefficient memory management,case1,Not Enough Space on every Inquiry · Issue ,ST,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly configured in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Open the GUI: Access the web UI.
4. Initiate a New Chat: Start a new chat session.
5. Enter a Query: Type a query and submit it.
6. Observe the Response: Notice if the AI's response is missing or incomplete.","['privateGPT.py', 'ingest.py']",['fail-stops']
212,AgentJ-WR/Private-GPT-Flask,https://github.com/AgentJ-WR/Private-GPT-Flask/tree/328bbc320a6dacc1a120cfcaf9c88e0fd001b381,resource contention,case1,Set n_threads drastically slow down privateGPT ,SL,"privateGPT.py
ingest.py","Ensure PrivateGPT is set up and the server is running.
Open the privateGPT.py file.
Set the n_threads parameter to a high value, such as n_threads=40, to use all CPU cores.
Save the changes.
Restart the PrivateGPT server.
Observe the server's performance and note if there is a drastic slowdown or significant delay in response times when answering questions.
Additional Steps:

Try setting n_threads=1 to use only 1 CPU core and observe if the performance issue persists.
Remove the n_threads parameter entirely from the privateGPT.py file and check if the answer speed improves and returns to normal.","['privateGPT.py', 'ingest.py']",['slower execution']
213,REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,missing llm input format validation,/,"In linux, an error occurs when the.doc file is uploaded and parsed. Therefore, the.doc file cannot be parsed",ST IC,"privateGPT.py
ingest.py","1.Set up Langchain-Chatchat: Ensure the project is correctly set up in your local environment.
2.Use Linux Environment: Make sure you are operating within a Linux environment.
3.Upload a .doc File: Try uploading a .doc file to the knowledge base.
4.Observe the Error: Notice the error message indicating that the .doc file format is not supported and the file cannot be parsed.","['privateGPT.py', 'ingest.py']","['incorrectness', 'fail-stops']"
214,REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,exceeding llm content limit,case1,1.too many tokens ,ST,"privateGPT.py
ingest.py","1.Set up PrivateGPT: Ensure the PrivateGPT project is correctly set up in your local environment.
2.Configure Environment Variables: Set MODEL_N_CTX and MODEL_N_BATCH in the .env file.
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Query: Input a simple query such as ""hi"".
5.Observe the Error: Notice the error message ""too many tokens"" indicating the context window is exceeded.","['privateGPT.py', 'ingest.py']",['fail-stops']
215,REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,incompatible llm output format,case1,Answers contain additional prompts with certain models ,IC,"privateGPT.py
ingest.py","1.Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2.Configure Environment Variables: Set the environment variables, including MODEL_PATH to a specific model path (e.g., models/vicuna_ggml-vicuna-7b-1.1/ggml-vic7b-uncensored-q4_0.bin).
3.Run PrivateGPT: Execute python privateGPT.py.
4.Enter a Query: Input a query such as ""What is a tree?"".
5.Observe the Response: Notice that the response contains additional prompts or unrelated names and characters.","['privateGPT.py', 'ingest.py']",['incorrectness']
216,REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,unclear context in prompt,case1,Sources are not being used,IC,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Use the GUI: Open the web UI and upload a document.
4. Ask a Question: Enter a question that should be answerable from the document.
5. Observe the Response: Note that the model does not utilize the supplied documents' information but instead relies on its own knowledge.","['privateGPT.py', 'ingest.py']",['incorrectness']
217,REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,lacking restrictions in prompt,case1,If a Document fails to process; ingesting should continue,"ST,UI","privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly set up in your local environment.
2. Configure the Environment: Set the `MAX_HISTORY` parameter to a specific value, such as 10.
3. Run PrivateGPT: Execute `python privateGPT.py`.
4. Open the chat interface in the web UI.
5. Enter a series of queries exceeding the `MAX_HISTORY` value.
6. Observe the chat history: Notice if older messages are not being removed as expected or if there is an issue with the history management.","['privateGPT.py', 'ingest.py']","['fail-stops', 'unfriendly user interface']"
218,REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,imprecise knowledge retrieval,/,ingest does not complete,"IC,SL","privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly configured in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Open the GUI: Access the web UI.
4. Attempt to Upload a File: Try uploading a file with a size greater than 50MB.
5. Observe the Behavior: Notice if an error occurs or if the upload fails.","['privateGPT.py', 'ingest.py']","['incorrectness', 'slower execution']"
219,REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,inefficient memory management,case1,Not Enough Space on every Inquiry · Issue ,ST,"privateGPT.py
ingest.py","1. Set up PrivateGPT: Ensure the project is correctly configured in your local environment.
2. Start the PrivateGPT server: Execute `python privateGPT.py`.
3. Open the GUI: Access the web UI.
4. Initiate a New Chat: Start a new chat session.
5. Enter a Query: Type a query and submit it.
6. Observe the Response: Notice if the AI's response is missing or incomplete.","['privateGPT.py', 'ingest.py']",['fail-stops']
220,REFRIED-BoolEANS/privatgpt-gui,https://github.com/REFRIED-BoolEANS/privatgpt-gui/tree/840675927b1ace596dcc2c8ed0f6f483805e33fe,resource contention,case1,Set n_threads drastically slow down privateGPT ,SL,"privateGPT.py
ingest.py","Ensure PrivateGPT is set up and the server is running.
Open the privateGPT.py file.
Set the n_threads parameter to a high value, such as n_threads=40, to use all CPU cores.
Save the changes.
Restart the PrivateGPT server.
Observe the server's performance and note if there is a drastic slowdown or significant delay in response times when answering questions.
Additional Steps:

Try setting n_threads=1 to use only 1 CPU core and observe if the performance issue persists.
Remove the n_threads parameter entirely from the privateGPT.py file and check if the answer speed improves and returns to normal.","['privateGPT.py', 'ingest.py']",['slower execution']
221,starsnatched/MemGPT-multimodal,https://github.com/starsnatched/MemGPT-multimodal/tree/82b9b74efa9ad2a04dc531b0ef77a9e229e86e00,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,"memgpt/cli/cli_load.py
memgpt/memory.py",Upload the pptx file.,"['memgpt/cli/cli_load.py', 'memgpt/memory.py']",['incorrectness']
222,starsnatched/MemGPT-multimodal,https://github.com/starsnatched/MemGPT-multimodal/tree/82b9b74efa9ad2a04dc531b0ef77a9e229e86e00,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"memgpt/server/rest_api/agents/memory.py
memgpt/memory.py
memgpt/interface.py
memgpt/constants.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""","['memgpt/constants.py', 'memgpt/memory.py', 'memgpt/server/rest_api/agents/memory.py', 'memgpt/interface.py']",['incorrectness']
223,starsnatched/MemGPT-multimodal,https://github.com/starsnatched/MemGPT-multimodal/tree/82b9b74efa9ad2a04dc531b0ef77a9e229e86e00,knowledge misalignment,/,Attaching data to agents is not working as expected,IC,"memgpt/models/embedding_response.py
memgpt/embeddings.py
memgpt/constants.py","1.Set up MemGPT:Ensure MemGPT is correctly set up and running in your local environment.
2.Modify Configuration:Open the config.yaml file located in the project directory.
Set the max_memory parameter to 1024MB and the memory_limit parameter to 512MB.
3.Run a Memory-Intensive Task:Execute a memory-intensive task that exceeds the memory_limit parameter. For example, run the command:python run_task.py --task heavy_computation
4.Monitor Memory Usage:Observe the memory usage of the MemGPT process. Use a tool like htop or psutil to monitor memory consumption.
5.Observe the Error:Note if an error is thrown indicating that the memory usage has exceeded the limit. The error message should be similar to:MemoryError: Exceeded memory limit of 512MB.
6.Check Logs:Review the application logs for any additional error messages or stack traces related to the memory limit.","['memgpt/constants.py', 'memgpt/models/embedding_response.py', 'memgpt/embeddings.py']",['incorrectness']
224,starsnatched/MemGPT-multimodal,https://github.com/starsnatched/MemGPT-multimodal/tree/82b9b74efa9ad2a04dc531b0ef77a9e229e86e00,improper text embedding,case1,Timeout errors on MemGPT embedding endpoint ,SL,"memgpt/models/embedding_response.py
memgpt/embeddings.py
memgpt/server/rest_api/openai_assistants/assistants.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Start MemGPT Server: Execute `python memgpt.py` or the relevant command to run the server.
3. Open the embedding endpoint in the API interface or client.
4. Send an embedding request to the endpoint.
5. Observe if a timeout error occurs or if the request fails to complete within the expected time frame.","['memgpt/models/embedding_response.py', 'memgpt/embeddings.py', 'memgpt/server/rest_api/openai_assistants/assistants.py']",['slower execution']
225,starsnatched/MemGPT-multimodal,https://github.com/starsnatched/MemGPT-multimodal/tree/82b9b74efa9ad2a04dc531b0ef77a9e229e86e00,inefficient memory management,/,memgpt server fails once context memory is full (when it calls archival_memory_insert) ,ST,"memgpt/server/rest_api/agents/memory.py
memgpt/memory.py
memgpt/interface.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure Memory Context: Set the memory context to a specific limit in the configuration file.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Input a series of queries or interactions to fill up the context memory.
6. Observe if the server fails or encounters issues when the context memory is full, especially during calls to `archival_memory_insert`.","['memgpt/memory.py', 'memgpt/server/rest_api/agents/memory.py', 'memgpt/interface.py']",['fail-stops']
226,starsnatched/MemGPT-multimodal,https://github.com/starsnatched/MemGPT-multimodal/tree/82b9b74efa9ad2a04dc531b0ef77a9e229e86e00,exceeding llm content limit,case1,Memory context limit exceeded ,ST,"memgpt/data_types.py
memgpt/constants.py
memgpt/metadata.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure Memory Context: Set or verify the memory context limit in the configuration file.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Input a series of queries or interactions that are designed to exceed the memory context limit.
6. Observe if a ""Memory context limit exceeded"" error occurs or if the system fails to handle the excess memory context properly.","['memgpt/constants.py', 'memgpt/metadata.py', 'memgpt/data_types.py']",['fail-stops']
227,goetzrobin/MemGPT,https://github.com/goetzrobin/MemGPT/tree/0e8918e79935d24210991d78da09bf0e4c796d2e,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,"memgpt/main.py
memgpt/cli/cli_load.py
memgpt/data_sources/connectors.py",Upload the pptx file.,"['memgpt/cli/cli_load.py', 'memgpt/data_sources/connectors.py', 'memgpt/main.py']",['incorrectness']
228,goetzrobin/MemGPT,https://github.com/goetzrobin/MemGPT/tree/0e8918e79935d24210991d78da09bf0e4c796d2e,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"memgpt/models/openai.py
paper_experiments/doc_qa_task/doc_qa.py
memgpt/models/chat_completion_response.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""","['memgpt/models/openai.py', 'memgpt/models/chat_completion_response.py']",['incorrectness']
229,goetzrobin/MemGPT,https://github.com/goetzrobin/MemGPT/tree/0e8918e79935d24210991d78da09bf0e4c796d2e,knowledge misalignment,/,Attaching data to agents is not working as expected,IC,"memgpt/models/embedding_response.py
memgpt/embeddings.py
memgpt/constants.py","1.Set up MemGPT:Ensure MemGPT is correctly set up and running in your local environment.
2.Modify Configuration:Open the config.yaml file located in the project directory.
Set the max_memory parameter to 1024MB and the memory_limit parameter to 512MB.
3.Run a Memory-Intensive Task:Execute a memory-intensive task that exceeds the memory_limit parameter. For example, run the command:python run_task.py --task heavy_computation
4.Monitor Memory Usage:Observe the memory usage of the MemGPT process. Use a tool like htop or psutil to monitor memory consumption.
5.Observe the Error:Note if an error is thrown indicating that the memory usage has exceeded the limit. The error message should be similar to:MemoryError: Exceeded memory limit of 512MB.
6.Check Logs:Review the application logs for any additional error messages or stack traces related to the memory limit.","['memgpt/constants.py', 'memgpt/models/embedding_response.py', 'memgpt/embeddings.py']",['incorrectness']
230,goetzrobin/MemGPT,https://github.com/goetzrobin/MemGPT/tree/0e8918e79935d24210991d78da09bf0e4c796d2e,improper text embedding,case1,Timeout errors on MemGPT embedding endpoint ,SL,"memgpt/models/embedding_response.py
memgpt/embeddings.py
memgpt/constants.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Start MemGPT Server: Execute `python memgpt.py` or the relevant command to run the server.
3. Open the embedding endpoint in the API interface or client.
4. Send an embedding request to the endpoint.
5. Observe if a timeout error occurs or if the request fails to complete within the expected time frame.","['memgpt/constants.py', 'memgpt/models/embedding_response.py', 'memgpt/embeddings.py']",['slower execution']
231,goetzrobin/MemGPT,https://github.com/goetzrobin/MemGPT/tree/0e8918e79935d24210991d78da09bf0e4c796d2e,inefficient memory management,/,memgpt server fails once context memory is full (when it calls archival_memory_insert) ,ST,"memgpt/server/rest_api/agents/memory.py
memgpt/memory.py
memgpt/interface.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure Memory Context: Set the memory context to a specific limit in the configuration file.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Input a series of queries or interactions to fill up the context memory.
6. Observe if the server fails or encounters issues when the context memory is full, especially during calls to `archival_memory_insert`.","['memgpt/memory.py', 'memgpt/server/rest_api/agents/memory.py', 'memgpt/interface.py']",['fail-stops']
232,goetzrobin/MemGPT,https://github.com/goetzrobin/MemGPT/tree/0e8918e79935d24210991d78da09bf0e4c796d2e,exceeding llm content limit,case1,Memory context limit exceeded ,ST,"memgpt/data_types.py
memgpt/constants.py","1. Set up MemGPT: Ensure the project is correctly set up in your local environment.
2. Configure Memory Context: Set or verify the memory context limit in the configuration file.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Input a series of queries or interactions that are designed to exceed the memory context limit.
6. Observe if a ""Memory context limit exceeded"" error occurs or if the system fails to handle the excess memory context properly.","['memgpt/constants.py', 'memgpt/data_types.py']",['fail-stops']
233,GreyDGL/PentestGPT,https://github.com/GreyDGL/PentestGPT/tree/a4e5361d76c880a9cf6f79f12e6cff90c8c01da3,insufficient history management,/,Test history is not properly handled,IC,"pentestgpt/utils/chatgpt.py
pentestgpt/utils/llm_api.py
pentestgpt/utils/APIs/gpt4all_api.py","1. Set up PentestGPT: Ensure the project is correctly set up in your local environment.
2. Configure Test History: Verify the configuration for handling test history.
3. Run PentestGPT: Execute `python pentestgpt.py` or the relevant command to start the server.
4. Open the test interface in the web UI.
5. Conduct a series of tests or interactions.
6. Check if the test history is recorded and managed correctly, and observe any issues or inconsistencies in how the history is handled or displayed.","['pentestgpt/utils/chatgpt.py', 'pentestgpt/utils/APIs/gpt4all_api.py', 'pentestgpt/utils/llm_api.py']",['incorrectness']
234,0xk1h0/PentestGPT,https://github.com/0xk1h0/PentestGPT/tree/a6edb3053ef1b3e0c36286306bc95cd8e6ceb026,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,"pentestgpt/utils/pentest_gpt.py
pentestgpt/utils/pentest_gpt_rebuilt.py",Upload the pptx file.,"['pentestgpt/utils/pentest_gpt_rebuilt.py', 'pentestgpt/utils/pentest_gpt.py']",['incorrectness']
235,0xk1h0/PentestGPT,https://github.com/0xk1h0/PentestGPT/tree/a6edb3053ef1b3e0c36286306bc95cd8e6ceb026,unclear context in prompt,/,Knowledge Base Q&A does not match a concise description of/problem,IC,"pentestgpt/utils/prompt_select.py
pentestgpt/prompts/prompt_class_v2.py
pentestgpt/utils/pentest_gpt.py","1.Set up: Ensure the project is correctly set up in your local environment.
2.Execute 'Knowledge Base Management - Create New Knowledge Base - Upload Files'.
3.Click 'Conversation - Select Knowledge Base Q&A - Choose Knowledge Base'.
4.Scroll to 'Input Box and Ask a Question'.
5.Observe the issue: 'No related documents found, the response is based on the model's own capability.'","['pentestgpt/utils/prompt_select.py', 'pentestgpt/utils/pentest_gpt.py', 'pentestgpt/prompts/prompt_class_v2.py']",['incorrectness']
236,0xk1h0/PentestGPT,https://github.com/0xk1h0/PentestGPT/tree/a6edb3053ef1b3e0c36286306bc95cd8e6ceb026,exceeding llm content limit,/,Memory context limit exceeded ,ST,"pentestgpt/utils/llm_api.py
pentestgpt/utils/chatgpt.py
pentestgpt/prompts/prompt_class_v1.py","1. Set up: Ensure the project is correctly set up in your local environment.
2. Configure Memory Context: Set or verify the memory context limit in the configuration file.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Input a series of queries or interactions that are designed to exceed the memory context limit.
6. Observe if a ""Memory context limit exceeded"" error occurs or if the system fails to handle the excess memory context properly.","['pentestgpt/prompts/prompt_class_v1.py', 'pentestgpt/utils/chatgpt.py', 'pentestgpt/utils/llm_api.py']",['fail-stops']
237,0xk1h0/PentestGPT,https://github.com/0xk1h0/PentestGPT/tree/a6edb3053ef1b3e0c36286306bc95cd8e6ceb026,insufficient history management,/,Test history is not properly handled,IC,"pentestgpt/utils/chatgpt.py
pentestgpt/utils/llm_api.py
pentestgpt/utils/APIs/gpt4all_api.py","1. Set up PentestGPT: Ensure the project is correctly set up in your local environment.
2. Configure Test History: Verify the configuration for handling test history.
3. Run PentestGPT: Execute `python pentestgpt.py` or the relevant command to start the server.
4. Open the test interface in the web UI.
5. Conduct a series of tests or interactions.
6. Check if the test history is recorded and managed correctly, and observe any issues or inconsistencies in how the history is handled or displayed.","['pentestgpt/utils/chatgpt.py', 'pentestgpt/utils/APIs/gpt4all_api.py', 'pentestgpt/utils/llm_api.py']",['incorrectness']
238,0xk1h0/PentestGPT,https://github.com/0xk1h0/PentestGPT/tree/a6edb3053ef1b3e0c36286306bc95cd8e6ceb026,sketchy error handling,case1,Add handler for repeated commands.,"IC,SL","pentestgpt/utils/chatgpt.py
pentestgpt/utils/llm_api.py","1.Set up PentestGPT: Ensure PentestGPT is correctly set up and running in your local environment following the setup instructions in the project's README.md.
2.Configure the Environment: Open the config.py file located in the project directory.Set the API_KEY parameter with your valid API key.Ensure other configuration parameters are set to their default values unless specified otherwise.
3.Initiate a Scan: Start a pentesting scan using the following command:python pentest.py --target example.com
4.Monitor the Output: Observe the output in the terminal and note if the scan process initiates correctly.
5.Trigger the Error: During the scan process, attempt to run a specific module or test that requires elevated privileges or specific network access. For example:python pentest.py --target example.com --module network_scan
6.Observe the Error: Note if an error occurs indicating a failure in the scan process. The error message should be similar to:PermissionError: [Errno 13] Permission denied
Alternatively, the error might relate to missing dependencies or incorrect configuration, such as:ModuleNotFoundError: No module named 'some_required_module'
7.Check Logs: Review the application logs and console output for any additional error messages or stack traces related to the failure.","['pentestgpt/utils/chatgpt.py', 'pentestgpt/utils/llm_api.py']","['incorrectness', 'slower execution']"
239,vmayoral/PentestGPT,https://github.com/vmayoral/PentestGPT/tree/58e80fa92dea24da3d4899aa625b7683100720da,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,"prompts/prompt_class_old.py
utils/pentest_gpt.py
prompts/prompt_class.py",Upload the pptx file.,"['prompts/prompt_class.py', 'utils/pentest_gpt.py', 'prompts/prompt_class_old.py']",['incorrectness']
240,vmayoral/PentestGPT,https://github.com/vmayoral/PentestGPT/tree/58e80fa92dea24da3d4899aa625b7683100720da,unclear context in prompt,/,Knowledge Base Q&A does not match a concise description of/problem,IC,"prompts/prompt_class_old.py
prompts/prompt_class.py","1.Set up: Ensure the project is correctly set up in your local environment.
2.Execute 'Knowledge Base Management - Create New Knowledge Base - Upload Files'.
3.Click 'Conversation - Select Knowledge Base Q&A - Choose Knowledge Base'.
4.Scroll to 'Input Box and Ask a Question'.
5.Observe the issue: 'No related documents found, the response is based on the model's own capability.'","['prompts/prompt_class.py', 'prompts/prompt_class_old.py']",['incorrectness']
241,vmayoral/PentestGPT,https://github.com/vmayoral/PentestGPT/tree/58e80fa92dea24da3d4899aa625b7683100720da,exceeding llm content limit,,Memory context limit exceeded ,ST,"utils/chatgpt_api.py
utils/chatgpt.py","1. Set up: Ensure the project is correctly set up in your local environment.
2. Configure Memory Context: Set or verify the memory context limit in the configuration file.
3. Run MemGPT: Execute `python memgpt.py` or the relevant command to start the server.
4. Open the chat or interaction interface.
5. Input a series of queries or interactions that are designed to exceed the memory context limit.
6. Observe if a ""Memory context limit exceeded"" error occurs or if the system fails to handle the excess memory context properly.","['utils/chatgpt_api.py', 'utils/chatgpt.py']",['fail-stops']
242,vmayoral/PentestGPT,https://github.com/vmayoral/PentestGPT/tree/58e80fa92dea24da3d4899aa625b7683100720da,insufficient history management,/,Test history is not properly handled,IC,"utils/chatgpt_browser.py
utils/chatgpt_api.py
utils/chatgpt.py","1. Set up PentestGPT: Ensure the project is correctly set up in your local environment.
2. Configure Test History: Verify the configuration for handling test history.
3. Run PentestGPT: Execute `python pentestgpt.py` or the relevant command to start the server.
4. Open the test interface in the web UI.
5. Conduct a series of tests or interactions.
6. Check if the test history is recorded and managed correctly, and observe any issues or inconsistencies in how the history is handled or displayed.","['utils/chatgpt_browser.py', 'utils/chatgpt_api.py', 'utils/chatgpt.py']",['incorrectness']
243,vmayoral/PentestGPT,https://github.com/vmayoral/PentestGPT/tree/58e80fa92dea24da3d4899aa625b7683100720da,sketchy error handling,case1,Add handler for repeated commands.,"IC,SL","utils/chatgpt_api.py
utils/chatgpt.py","1.Set up PentestGPT: Ensure PentestGPT is correctly set up and running in your local environment following the setup instructions in the project's README.md.
2.Configure the Environment: Open the config.py file located in the project directory.Set the API_KEY parameter with your valid API key.Ensure other configuration parameters are set to their default values unless specified otherwise.
3.Initiate a Scan: Start a pentesting scan using the following command:python pentest.py --target example.com
4.Monitor the Output: Observe the output in the terminal and note if the scan process initiates correctly.
5.Trigger the Error: During the scan process, attempt to run a specific module or test that requires elevated privileges or specific network access. For example:python pentest.py --target example.com --module network_scan
6.Observe the Error: Note if an error occurs indicating a failure in the scan process. The error message should be similar to:PermissionError: [Errno 13] Permission denied
Alternatively, the error might relate to missing dependencies or incorrect configuration, such as:ModuleNotFoundError: No module named 'some_required_module'
7.Check Logs: Review the application logs and console output for any additional error messages or stack traces related to the failure.","['utils/chatgpt_api.py', 'utils/chatgpt.py']","['incorrectness', 'slower execution']"
244,ranfysvalle02/chatdocs-mdb,https://github.com/ranfysvalle02/chatdocs-mdb/tree/6322d0ea03d2a3a14fbfaa7ca9f12104e733a72f,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,chatdocs/chat.py,Upload the pptx file.,['chatdocs/chat.py'],['incorrectness']
245,ranfysvalle02/chatdocs-mdb,https://github.com/ranfysvalle02/chatdocs-mdb/tree/6322d0ea03d2a3a14fbfaa7ca9f12104e733a72f,lacking restrictions in prompt,case1,Limiting input length to prevent out of memory issues,"ST,UI",chatdocs/chat.py,"1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Configure Input Length Limit: Set a specific limit for input length in the configuration file.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the input interface in the web UI.
5. Enter a lengthy input that exceeds the configured limit.
6. Observe if the system properly enforces the input length limit and prevents out-of-memory issues.",['chatdocs/chat.py'],"['fail-stops', 'unfriendly user interface']"
246,ranfysvalle02/chatdocs-mdb,https://github.com/ranfysvalle02/chatdocs-mdb/tree/6322d0ea03d2a3a14fbfaa7ca9f12104e733a72f,unclear context in prompt,case1,Query giving wrong citations and answers out of the document,IC,chatdocs/chat.py,"1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded document.
6. Observe the response to check if it includes wrong citations or answers that are not from the document.",['chatdocs/chat.py'],['incorrectness']
247,ranfysvalle02/chatdocs-mdb,https://github.com/ranfysvalle02/chatdocs-mdb/tree/6322d0ea03d2a3a14fbfaa7ca9f12104e733a72f,knowledge misalignment,/,Error when parquet files get too big / function for splitting?,ST,"chatdocs/embeddings.py
chatdocs/add.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Prepare a large Parquet file for testing.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Upload the large Parquet file using the web UI.
5. Observe if an error occurs when handling the large Parquet file.
6. Check if there is a function or option for splitting the Parquet file to avoid errors.","['chatdocs/add.py', 'chatdocs/embeddings.py']",['fail-stops']
248,ranfysvalle02/chatdocs-mdb,https://github.com/ranfysvalle02/chatdocs-mdb/tree/6322d0ea03d2a3a14fbfaa7ca9f12104e733a72f,imprecise knowledge retrieval,/,"Asking a question gives ""Index not found, please create an instance before querying""",ST,"chatdocs/embeddings.py
chatdocs/add.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a question related to the content of the uploaded document.For example, ""Summarize the key points of the document.""
6. Observe if the response includes the error message ""Index not found, please create an instance before querying.""or ""ValueError: invalid literal for int() with base 10: 'some_value'""","['chatdocs/add.py', 'chatdocs/embeddings.py']",['fail-stops']
249,ranfysvalle02/chatdocs-mdb,https://github.com/ranfysvalle02/chatdocs-mdb/tree/6322d0ea03d2a3a14fbfaa7ca9f12104e733a72f,exceeding llm content limit,case1,Responses get cut off in the middle.,IC,"chatdocs/llms.py
chatdocs/ui.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded document.
6. Observe the response to check if it gets cut off in the middle.","['chatdocs/llms.py', 'chatdocs/ui.py']",['incorrectness']
250,Vidminas/chatdocs-streamlit,https://github.com/Vidminas/chatdocs-streamlit/tree/cf4053a1b20116fb0d05e64778e3c4c28b902f2a,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,"chatdocs/chat.py
chatdocs/pages/embeddings_viz.py
chatdocs/document_loaders/nougat_loader.py",Upload the pptx file.,"['chatdocs/pages/embeddings_viz.py', 'chatdocs/document_loaders/nougat_loader.py', 'chatdocs/chat.py']",['incorrectness']
251,Vidminas/chatdocs-streamlit,https://github.com/Vidminas/chatdocs-streamlit/tree/cf4053a1b20116fb0d05e64778e3c4c28b902f2a,lacking restrictions in prompt,case1,Limiting input length to prevent out of memory issues,"ST,UI",chatdocs/ui.py,"1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Configure Input Length Limit: Set a specific limit for input length in the configuration file.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the input interface in the web UI.
5. Enter a lengthy input that exceeds the configured limit.
6. Observe if the system properly enforces the input length limit and prevents out-of-memory issues.",['chatdocs/ui.py'],"['fail-stops', 'unfriendly user interface']"
252,Vidminas/chatdocs-streamlit,https://github.com/Vidminas/chatdocs-streamlit/tree/cf4053a1b20116fb0d05e64778e3c4c28b902f2a,unclear context in prompt,case1,Query giving wrong citations and answers out of the document,IC,chatdocs/ui.py,"1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded document.
6. Observe the response to check if it includes wrong citations or answers that are not from the document.",['chatdocs/ui.py'],['incorrectness']
253,Vidminas/chatdocs-streamlit,https://github.com/Vidminas/chatdocs-streamlit/tree/cf4053a1b20116fb0d05e64778e3c4c28b902f2a,knowledge misalignment,/,Error when parquet files get too big / function for splitting?,ST,"chatdocs/pages/embeddings_viz.py
chatdocs/embeddings.py
chatdocs/vectorstores.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Prepare a large Parquet file for testing.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Upload the large Parquet file using the web UI.
5. Observe if an error occurs when handling the large Parquet file.
6. Check if there is a function or option for splitting the Parquet file to avoid errors.","['chatdocs/vectorstores.py', 'chatdocs/pages/embeddings_viz.py', 'chatdocs/embeddings.py']",['fail-stops']
254,Vidminas/chatdocs-streamlit,https://github.com/Vidminas/chatdocs-streamlit/tree/cf4053a1b20116fb0d05e64778e3c4c28b902f2a,imprecise knowledge retrieval,/,"Asking a question gives ""Index not found, please create an instance before querying""",ST,"chatdocs/ui.py
chatdocs/chains.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a question related to the content of the uploaded document.For example, ""Summarize the key points of the document.""
6. Observe if the response includes the error message ""Index not found, please create an instance before querying.""or ""ValueError: invalid literal for int() with base 10: 'some_value'""","['chatdocs/chains.py', 'chatdocs/ui.py']",['fail-stops']
255,Vidminas/chatdocs-streamlit,https://github.com/Vidminas/chatdocs-streamlit/tree/cf4053a1b20116fb0d05e64778e3c4c28b902f2a,exceeding llm content limit,case1,Responses get cut off in the middle.,IC,"chatdocs/chat.py
chatdocs/ui.py
chatdocs/llms.py","1. Set up ChatDocs: Ensure the project is correctly set up in your local environment.
2. Upload a document for querying.
3. Run ChatDocs: Execute `python chatdocs.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded document.
6. Observe the response to check if it gets cut off in the middle.","['chatdocs/ui.py', 'chatdocs/llms.py', 'chatdocs/chat.py']",['incorrectness']
256,iMagist486/ElasticSearch-Langchain-Chatglm2,https://github.com/iMagist486/ElasticSearch-Langchain-Chatglm2/tree/304d3d204a00782a0078fd76be232fa5802f4307,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"model/base.py
model/chatglm_llm.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""","['model/chatglm_llm.py', 'model/base/base.py']",['incorrectness']
257,iMagist486/ElasticSearch-Langchain-Chatglm2,https://github.com/iMagist486/ElasticSearch-Langchain-Chatglm2/tree/304d3d204a00782a0078fd76be232fa5802f4307,missing llm input format validation,/,This project can't accept images read from PDF. ,IC,web.py,"1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document that contains images.
3. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
4. Open the chat interface in the web UI.
5. Enter a query that references the images in the PDF.
6. Observe if the response fails to include or reference the images from the PDF, indicating that the project cannot process images read from PDF documents.(for example,OSError: [Errno 24] Too many open files)
7.Check Logs and Output: Review the application logs and console output for any additional error messages or stack traces that provide more details about the failure.",['web.py'],['incorrectness']
258,iMagist486/ElasticSearch-Langchain-Chatglm2,https://github.com/iMagist486/ElasticSearch-Langchain-Chatglm2/tree/304d3d204a00782a0078fd76be232fa5802f4307,exceeding llm content limit,/,"PineconeError: Error, message length too large: found 5453452 bytes, the limit is: 4194304 bytes",IC,"model/chatglm_llm.py
web.py","1. Set up GPT4 PDF Chatbot LangChain: Ensure the project is correctly set up in your local environment.
2. Configure Pinecone: Ensure Pinecone is properly set up and integrated.
3. Upload a large PDF document or dataset that could exceed typical message size limits.
4. Run the Chatbot: Execute `python app.py` or the relevant command to start the server.
5. Open the chat interface in the web UI.
6. Enter a query that processes the large PDF document or dataset.
7. Observe if a PineconeError occurs, specifically an error message indicating that the message length is too large, such as ""found 5453452 bytes, the limit is: 4194304 bytes.""",['model/chatglm_llm.py'],['incorrectness']
259,alejandro-ao/ask-multiple-pdfs,https://github.com/alejandro-ao/ask-multiple-pdfs/tree/362e85213a01d73772d5319fb9819e026ecbe8a7,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,app.py,"1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""",['app.py'],['incorrectness']
260,alejandro-ao/ask-multiple-pdfs,https://github.com/alejandro-ao/ask-multiple-pdfs/tree/362e85213a01d73772d5319fb9819e026ecbe8a7,unnecessary llm output,/,The last sentence in the generated output is repeated multiple times. ,SL,app.py,"1. Set up Ask Multiple PDFs: Ensure the project is correctly set up in your local environment.
2. Upload multiple PDF documents for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that requires generating a detailed response.
6. Observe the generated output to check if the last sentence is repeated multiple times.",['app.py'],['slower execution']
261,alejandro-ao/ask-multiple-pdfs,https://github.com/alejandro-ao/ask-multiple-pdfs/tree/362e85213a01d73772d5319fb9819e026ecbe8a7,exceeding llm content limit,/,"InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 6777 tokens. Please reduce the length of the messages",ST,app.py,"1. Set up Ask Multiple PDFs: Ensure the project is correctly set up in your local environment.
2. Upload multiple PDF documents for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a very detailed or lengthy query that is likely to exceed the model's maximum context length.
6. Observe if an `InvalidRequestError` occurs with a message indicating that the model's maximum context length of 4097 tokens has been exceeded by your messages, resulting in 6777 tokens.",['app.py'],['fail-stops']
262,alejandro-ao/ask-multiple-pdfs,https://github.com/alejandro-ao/ask-multiple-pdfs/tree/362e85213a01d73772d5319fb9819e026ecbe8a7,missing llm input format validation,/,Every time you have to reload the PDF file,SL,app.py,"1. Set up Ask Multiple PDFs: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the uploaded PDF document.
6. After receiving the response, navigate away from the query interface or refresh the page.
7. Return to the query interface.
8. Attempt to enter another query related to the initially uploaded PDF document.
9. Observe if you need to reload the PDF file every time you want to query it again.",['app.py'],['slower execution']
263,aigeek0x0/chat-with-multiple-pdfs-streamlit-langchain-faiss-openai,https://github.com/aigeek0x0/chat-with-multiple-pdfs-streamlit-langchain-faiss-openai/tree/3cc2b0ac1522d6a78a998759ea212052ddefb3b7,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,app.py,"1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""",['app.py'],['incorrectness']
264,aigeek0x0/chat-with-multiple-pdfs-streamlit-langchain-faiss-openai,https://github.com/aigeek0x0/chat-with-multiple-pdfs-streamlit-langchain-faiss-openai/tree/3cc2b0ac1522d6a78a998759ea212052ddefb3b7,unnecessary llm output,/,The last sentence in the generated output is repeated multiple times. ,SL,app.py,"1. Set up Ask Multiple PDFs: Ensure the project is correctly set up in your local environment.
2. Upload multiple PDF documents for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query that requires generating a detailed response.
6. Observe the generated output to check if the last sentence is repeated multiple times.",['app.py'],['slower execution']
265,aigeek0x0/chat-with-multiple-pdfs-streamlit-langchain-faiss-openai,https://github.com/aigeek0x0/chat-with-multiple-pdfs-streamlit-langchain-faiss-openai/tree/3cc2b0ac1522d6a78a998759ea212052ddefb3b7,exceeding llm content limit,/,"InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 6777 tokens. Please reduce the length of the messages",ST,app.py,"1. Set up Ask Multiple PDFs: Ensure the project is correctly set up in your local environment.
2. Upload multiple PDF documents for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a very detailed or lengthy query that is likely to exceed the model's maximum context length.
6. Observe if an `InvalidRequestError` occurs with a message indicating that the model's maximum context length of 4097 tokens has been exceeded by your messages, resulting in 6777 tokens.",['app.py'],['fail-stops']
266,aigeek0x0/chat-with-multiple-pdfs-streamlit-langchain-faiss-openai,https://github.com/aigeek0x0/chat-with-multiple-pdfs-streamlit-langchain-faiss-openai/tree/3cc2b0ac1522d6a78a998759ea212052ddefb3b7,missing llm input format validation,/,Every time you have to reload the PDF file,SL,app.py,"1. Set up Ask Multiple PDFs: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the uploaded PDF document.
6. After receiving the response, navigate away from the query interface or refresh the page.
7. Return to the query interface.
8. Attempt to enter another query related to the initially uploaded PDF document.
9. Observe if you need to reload the PDF file every time you want to query it again.",['app.py'],['slower execution']
267,mayooear/private-chatbot-mpt30b-langchain,https://github.com/mayooear/private-chatbot-mpt30b-langchain/tree/dbb888a2d5f1eaf6256ffab53e4ec0b766a86a3f,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"utils.py
chat.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""","['chat.py', 'utils.py']",['incorrectness']
268,mayooear/private-chatbot-mpt30b-langchain,https://github.com/mayooear/private-chatbot-mpt30b-langchain/tree/dbb888a2d5f1eaf6256ffab53e4ec0b766a86a3f,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,"chat.py
question_answer_docs.py",Upload the pptx file.,"['chat.py', 'question_answer_docs.py']",['incorrectness']
269,mayooear/private-chatbot-mpt30b-langchain,https://github.com/mayooear/private-chatbot-mpt30b-langchain/tree/dbb888a2d5f1eaf6256ffab53e4ec0b766a86a3f,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,"chat.py
ingest.py","

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk","['chat.py', 'ingest.py']",['fail-stops']
270,c0sogi/LLMChat,https://github.com/c0sogi/LLMChat/tree/b0fe554ca2327d2dc43dc819934b7973e662ffdb,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"app/models/llms.py
app/models/completion_models.py
app/utils/chat/commands/prompt.py
app/models/base_models.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""",['app/viewmodels/base_models.py'],['incorrectness']
271,c0sogi/LLMChat,https://github.com/c0sogi/LLMChat/tree/b0fe554ca2327d2dc43dc819934b7973e662ffdb,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,"app/utils/chat/text_generations/__init__.py
app/models/base_models.py
app/models/llms.py",Upload the pptx file.,['app/viewmodels/base_models.py'],['incorrectness']
272,c0sogi/LLMChat,https://github.com/c0sogi/LLMChat/tree/b0fe554ca2327d2dc43dc819934b7973e662ffdb,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,"app/utils/auth/token.py
app/models/base_models.py","

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk","['app/viewmodels/base_models.py', 'app/utils/auth/token.py']",['fail-stops']
273,dotvignesh/PDFChat,https://github.com/dotvignesh/PDFChat/tree/bb46824b7835386d39f76cd0c70b427dc42997c9,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,app.py,"1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""",['app.py'],['incorrectness']
274,dotvignesh/PDFChat,https://github.com/dotvignesh/PDFChat/tree/bb46824b7835386d39f76cd0c70b427dc42997c9,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,app.py,Upload the pptx file.,['app.py'],['incorrectness']
275,dotvignesh/PDFChat,https://github.com/dotvignesh/PDFChat/tree/bb46824b7835386d39f76cd0c70b427dc42997c9,exceeding llm content limit,/,can't upload multiples files and ask Q&A from.,ST,app.py,"1. Set up PDFChat: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the upload interface in the web UI.
4. Attempt to upload multiple PDF files.
5. Enter a query that requires information from the content of the uploaded PDF files.
6. Observe if the application fails to handle multiple file uploads and if you encounter issues when asking questions based on the content of the multiple files.",['app.py'],['fail-stops']
276,yvann-hub/Robby-chatbot,https://github.com/yvann-hub/Robby-chatbot/tree/5beb6894c8ba0d0b67188ba2be568a2a22a00491,exceeding llm content limit,case1,can't upload multiples csv files for the chatbot,ST,"src/modules/robby_sheet/table_tool.py
src/modules/chatbot.py","1. Set up Robby-chatbot: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the upload interface in the web UI.
4. Attempt to upload multiple CSV files.
5. Observe if the application fails to handle the upload of multiple CSV files for the chatbot.",['src/modules/chatbot.py'],['fail-stops']
277,yvann-hub/Robby-chatbot,https://github.com/yvann-hub/Robby-chatbot/tree/5beb6894c8ba0d0b67188ba2be568a2a22a00491,imprecise knowledge retrieval,/,when the embeded file size increases the query time also increases dramatically,SL,"tuto_chatbot_csv.py
src/modules/embedder.py","1. Set up Robby-chatbot: Ensure the project is correctly set up in your local environment.
2. Prepare a series of files of varying sizes to be used for embedding.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the upload interface in the web UI.
5. Upload a small file and measure the query time by entering a relevant query.
6. Upload a medium-sized file and measure the query time by entering a relevant query.
7. Upload a large file and measure the query time by entering a relevant query.
8. Observe if the query time increases dramatically with the size of the embedded file.","['src/modules/embedder.py', 'src/tuto_chatbot_csv.py']",['slower execution']
278,yvann-hub/Robby-chatbot,https://github.com/yvann-hub/Robby-chatbot/tree/5beb6894c8ba0d0b67188ba2be568a2a22a00491,missing llm input format validation,case1,Problem loading XLSX file,IC,tuto_chatbot_csv.py,"1. Set up Robby-chatbot: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the upload interface in the web UI.
4. Attempt to upload an XLSX file.
5. Observe if there is any problem or error message during the loading process of the XLSX file.(for example, TypeError: 'NoneType' object is not callable)",['src/tuto_chatbot_csv.py'],['incorrectness']
279,yvann-hub/Robby-chatbot,https://github.com/yvann-hub/Robby-chatbot/tree/5beb6894c8ba0d0b67188ba2be568a2a22a00491,unclear context in prompt,/,Limit the answers to the file only.,IC,"src/modules/layout.py
src/modules/history.py","1. Set up Robby-chatbot: Ensure the project is correctly set up in your local environment.
2. Upload a file containing specific information for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded file.
6. Observe the response to ensure it is limited to the information within the uploaded file only, without including external or unrelated information.(for example, AttributeError: 'Chatbot' object has no attribute 'tell_joke')","['src/modules/layout.py', 'src/modules/history.py']",['incorrectness']
280,gabacode/ChatBot-PDF,https://github.com/gabacode/ChatBot-PDF/tree/320476ac687db4ea0e9051b4b25ef6904d3a014d,exceeding llm content limit,case1,can't upload multiples csv files for the chatbot,ST,modules/chatbot.py,"1. Set up: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the upload interface in the web UI.
4. Attempt to upload multiple CSV files.
5. Observe if the application fails to handle the upload of multiple CSV files for the chatbot.",['modules/chatbot.py'],['fail-stops']
281,gabacode/ChatBot-PDF,https://github.com/gabacode/ChatBot-PDF/tree/320476ac687db4ea0e9051b4b25ef6904d3a014d,imprecise knowledge retrieval,/,when the embeded file size increases the query time also increases dramatically,SL,"modules/chatbot.py
modules/embedder.py","1. Set up: Ensure the project is correctly set up in your local environment.
2. Prepare a series of files of varying sizes to be used for embedding.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the upload interface in the web UI.
5. Upload a small file and measure the query time by entering a relevant query.
6. Upload a medium-sized file and measure the query time by entering a relevant query.
7. Upload a large file and measure the query time by entering a relevant query.
8. Observe if the query time increases dramatically with the size of the embedded file.","['modules/chatbot.py', 'modules/embedder.py']",['slower execution']
282,gabacode/ChatBot-PDF,https://github.com/gabacode/ChatBot-PDF/tree/320476ac687db4ea0e9051b4b25ef6904d3a014d,missing llm input format validation,case1,Problem loading XLSX file,IC,modules/chatbot.py,"1. Set up: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the upload interface in the web UI.
4. Attempt to upload an XLSX file.
5. Observe if there is any problem or error message during the loading process of the XLSX file.(for example, TypeError: 'NoneType' object is not callable)",['modules/chatbot.py'],['incorrectness']
283,gabacode/ChatBot-PDF,https://github.com/gabacode/ChatBot-PDF/tree/320476ac687db4ea0e9051b4b25ef6904d3a014d,unclear context in prompt,/,Limit the answers to the file only.,IC,modules/history.py,"1. Set up: Ensure the project is correctly set up in your local environment.
2. Upload a file containing specific information for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded file.
6. Observe the response to ensure it is limited to the information within the uploaded file only, without including external or unrelated information.(for example, AttributeError: 'Chatbot' object has no attribute 'tell_joke')",['modules/history.py'],['incorrectness']
284,chinesewebman/doc-chatbot,https://github.com/chinesewebman/doc-chatbot/tree/d17caf9f0414bc7bc802774de1f13c5d746966fb,exceeding llm content limit,case1,can't upload multiples csv files for the chatbot,ST,"src/modules/chatbot.py
src/modules/sidebar.py","1. Set up: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the upload interface in the web UI.
4. Attempt to upload multiple CSV files.
5. Observe if the application fails to handle the upload of multiple CSV files for the chatbot.","['src/modules/sidebar.py', 'src/modules/chatbot.py']",['fail-stops']
285,chinesewebman/doc-chatbot,https://github.com/chinesewebman/doc-chatbot/tree/d17caf9f0414bc7bc802774de1f13c5d746966fb,imprecise knowledge retrieval,/,when the embeded file size increases the query time also increases dramatically,SL,"src/modules/chatbot.py
src/modules/embedder.py","1. Set up: Ensure the project is correctly set up in your local environment.
2. Prepare a series of files of varying sizes to be used for embedding.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the upload interface in the web UI.
5. Upload a small file and measure the query time by entering a relevant query.
6. Upload a medium-sized file and measure the query time by entering a relevant query.
7. Upload a large file and measure the query time by entering a relevant query.
8. Observe if the query time increases dramatically with the size of the embedded file.","['src/modules/embedder.py', 'src/modules/chatbot.py']",['slower execution']
286,chinesewebman/doc-chatbot,https://github.com/chinesewebman/doc-chatbot/tree/d17caf9f0414bc7bc802774de1f13c5d746966fb,missing llm input format validation,case1,Problem loading XLSX file,IC,"src/modules/layout.py
src/modules/utils.py","1. Set up: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the upload interface in the web UI.
4. Attempt to upload an XLSX file.
5. Observe if there is any problem or error message during the loading process of the XLSX file.(for example, TypeError: 'NoneType' object is not callable)","['src/modules/layout.py', 'src/modules/utils.py']",['incorrectness']
287,chinesewebman/doc-chatbot,https://github.com/chinesewebman/doc-chatbot/tree/d17caf9f0414bc7bc802774de1f13c5d746966fb,unclear context in prompt,/,Limit the answers to the file only.,IC,"src/modules/layout.py
src/modules/chatbot.py","1. Set up: Ensure the project is correctly set up in your local environment.
2. Upload a file containing specific information for querying.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded file.
6. Observe the response to ensure it is limited to the information within the uploaded file only, without including external or unrelated information.(for example, AttributeError: 'Chatbot' object has no attribute 'tell_joke')","['src/modules/layout.py', 'src/modules/chatbot.py']",['incorrectness']
288,Doriandarko/BabyAGIChatGPT,https://github.com/Doriandarko/BabyAGIChatGPT/tree/174491cbbc09d3f6ef0ec511e9e3b94e2615ffa0,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"openapi.yaml
main.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""",['main.py'],['incorrectness']
289,Doriandarko/BabyAGIChatGPT,https://github.com/Doriandarko/BabyAGIChatGPT/tree/174491cbbc09d3f6ef0ec511e9e3b94e2615ffa0,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,"openapi.yaml
main.py",Upload the pptx file.,['main.py'],['incorrectness']
290,Doriandarko/BabyAGIChatGPT,https://github.com/Doriandarko/BabyAGIChatGPT/tree/174491cbbc09d3f6ef0ec511e9e3b94e2615ffa0,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,"openapi.yaml
main.py","

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk",['main.py'],['fail-stops']
291,benjaminearlevans/Chatgpt-babyagi-plugin,https://github.com/benjaminearlevans/Chatgpt-babyagi-plugin/tree/771a5ba6e6b69c30020da7a139ccec868b4a3dc7,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"openapi.yaml
main.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""",['main.py'],['incorrectness']
292,benjaminearlevans/Chatgpt-babyagi-plugin,https://github.com/benjaminearlevans/Chatgpt-babyagi-plugin/tree/771a5ba6e6b69c30020da7a139ccec868b4a3dc7,missing llm input format validation,/,The analysis of PPT documents needs optimization,IC,"openapi.yaml
main.py",Upload the pptx file.,['main.py'],['incorrectness']
293,benjaminearlevans/Chatgpt-babyagi-plugin,https://github.com/benjaminearlevans/Chatgpt-babyagi-plugin/tree/771a5ba6e6b69c30020da7a139ccec868b4a3dc7,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,"openapi.yaml
main.py","1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk",['main.py'],['fail-stops']
294,junruxiong/IncarnaMind,https://github.com/junruxiong/IncarnaMind/tree/75564a3bf08d0006387889cdc9a76fc509ededd8,missing llm input format validation,/,pdf with only pictures and no text will cause the list index out of range,IC,"main.py
convo_qa_chain.py","1. Set up IncarnaMind: Ensure the project is correctly set up in your local environment.
2. Upload a PDF document that contains only pictures and no text.
3. Run the application: Execute `python app.py` or the relevant command to start the server.
4. Open the query interface in the web UI.
5. Enter a query related to the content of the uploaded PDF document.
6. Observe if a ""list index out of range"" error occurs when processing the PDF with only pictures and no text.(for example, AttributeError: 'NoneType' object has no attribute 'some_method')","['main.py', 'convo_qa_chain.py']",['incorrectness']
295,junruxiong/IncarnaMind,https://github.com/junruxiong/IncarnaMind/tree/75564a3bf08d0006387889cdc9a76fc509ededd8,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"toolkit/prompts.py
convo_qa_chain.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""",['toolkit/prompts.py'],['incorrectness']
296,junruxiong/IncarnaMind,https://github.com/junruxiong/IncarnaMind/tree/75564a3bf08d0006387889cdc9a76fc509ededd8,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,"toolkit/together_api_llm.py
docs2db.py","

""1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk",['toolkit/together_api_llm.py'],['fail-stops']
297,Codium-ai/pr-agent,https://github.com/Codium-ai/pr-agent/tree/41166dc271039f9952c8086005f6c4b242021221,missing llm input format validation,/,File type detection for code documentation ,ST,"pr_agent/algo/utils.py
pr_agent/servers/bitbucket_app.py
","1. Set up PR-Agent: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the code documentation interface in the web UI.
4. Upload various types of code files (e.g., .py, .js, .java, .cpp).
5. Observe if the application correctly detects and categorizes the file types for code documentation purposes.(for example, ValueError: Invalid URL 'None')","['pr_agent/servers/bitbucket_app.py', 'pr_agent/algo/utils.py']",['fail-stops']
298,Codium-ai/pr-agent,https://github.com/Codium-ai/pr-agent/tree/41166dc271039f9952c8086005f6c4b242021221,unclear context in prompt,/,The LLM's answers sometimes contradict the truth or the content of the uploaded file,IC,"pr_agent/algo/token_handler.py
pr_agent/tools/pr_questions.py","1.In the the application, select a character to converse with.
2.Ask the character vague or ambiguous questions/overly complex or contradictory Information
like ""Explain what it is."" or ""If the sun were blue, what color would the moon be?""","['pr_agent/algo/token_handler.py', 'pr_agent/tools/pr_questions.py']",['incorrectness']
299,Codium-ai/pr-agent,https://github.com/Codium-ai/pr-agent/tree/41166dc271039f9952c8086005f6c4b242021221,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,"pr_agent/algo/token_handler.py
pr_agent/algo/utils.py","1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk","['pr_agent/algo/token_handler.py', 'pr_agent/algo/utils.py']",['fail-stops']
300,SolaceDev/pr-agent,https://github.com/SolaceDev/pr-agent/tree/5c768572233adb5fb9eadef21a184410b30b5603,missing llm input format validation,/,File type detection for code documentation ,ST,"pr_agent/algo/utils.py
pr_agent/algo/ai_handlers/langchain_ai_handler.py","1. Set up PR-Agent: Ensure the project is correctly set up in your local environment.
2. Run the application: Execute `python app.py` or the relevant command to start the server.
3. Open the code documentation interface in the web UI.
4. Upload various types of code files (e.g., .py, .js, .java, .cpp).
5. Observe if the application correctly detects and categorizes the file types for code documentation purposes.(for example, ValueError: Invalid URL 'None')","['pr_agent/algo/utils.py', 'pr_agent/algo/ai_handlers/langchain_ai_handler.py']",['fail-stops']
301,SolaceDev/pr-agent,https://github.com/SolaceDev/pr-agent/tree/5c768572233adb5fb9eadef21a184410b30b5603,exceeding llm content limit,/,Requested tokens exceed context window of 2048,ST,pr_agent/algo/token_handler.py,"1. Set up the application according to the README.md of this project
2. Upload a text document in the application's chat UI. Wait for multiple rounds of processing, and we will receive the following error:
‘...
Regarding completion_chunk",['pr_agent/algo/token_handler.py'],['fail-stops']
